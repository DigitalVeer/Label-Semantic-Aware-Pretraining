{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "---\n",
    "\n",
    "Here, we preprocess our data, and use 'label denoising' ask our masking strategy for pretraining. The idea is to take an utterance, and a label, and create a \"input\" and \"target\" associated with each one, with the pretraining object being to predict the masked token. For example:\n",
    "\n",
    "```\n",
    "Input: \"I am looking to book a flight from New York to Iceland. <MASK>\"\n",
    "Output: \"<MASK> Book Flight.\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "---\n",
    "We use the Author's settings for the most part, and utilize the same helper functions they did, which come from the TensorFlow Preprocessing Code.\n",
    "\n",
    "* https://github.com/amazon-science/label-aware-pretrain/blob/main/models/preprocessor.py  \n",
    "* https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py\n",
    "\n",
    "The different masking strategies are as follows:\n",
    "\n",
    "* **Label Denoising**: Mask the label, and predict the masked token.\n",
    "* **Intent Classification**: Add a prefix to each unmasked input, and predict the utterance.\n",
    "\n",
    "The args class makes it simple to rerun the preprocessing stage with different settings in this particular notebook. The args class is as follows:\n",
    "\n",
    "```\n",
    "Args(\n",
    "    dataset        # Location of the dataset\n",
    "    seed           # Random seed\n",
    "    labelsemantics # Masking strategy to use\n",
    "    tokenizer      # Tokenizer to use\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    \"\"\"Arguments for pretraining\"\"\"\n",
    "\n",
    "    dataset: str\n",
    "    seed: int\n",
    "    labelsemantics: str\n",
    "    tokenizer: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.dataset.endswith(\".json\")\n",
    "        assert self.labelsemantics in [\"random_denoising\", \"intent_classification\", \"label_denoising\"]\n",
    "\n",
    "args = Args(\n",
    "    dataset         = \"../data/pretraining/dataset/json/combined.json\",\n",
    "    seed            = 1248, \n",
    "    labelsemantics  = \"label_denoising\",\n",
    "    tokenizer       = \"t5-base\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handler\n",
    "---\n",
    "Below, we implement our own helper class to handle with reading the raw data from a json and writing it out to a formatted json for pre-training. The below class also helps with tokenization, cleaning up any unsanitized data, and acts as a parent class for the preprocessor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, PreTrainedTokenizerBase\n",
    "import torch, json\n",
    "\n",
    "@dataclass\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    DataHandler class for preprocessing data for pretraining. Responsible for tokenization and writing to file.\n",
    "    \"\"\"\n",
    "\n",
    "    args: Args\n",
    "    punc: tuple \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.tokenizer : PreTrainedTokenizerBase = T5Tokenizer.from_pretrained( self.args.tokenizer )\n",
    "\n",
    "    def to_dict(self, text, include_eos=True):\n",
    "        target = self.tokenizer.encode(text) if include_eos else self.tokenizer.encode(text)[:-1]\n",
    "        return {'inputs': \"\",\n",
    "                'targets': torch.tensor(target)}\n",
    "\n",
    "    def clean_str( self, txt ):\n",
    "        str = txt.strip()\n",
    "        if not str.endswith( self.punc ):\n",
    "            str += \".\"\n",
    "        return str\n",
    "\n",
    "    def load_data(self, in_file):\n",
    "        utterances, intents = [], []\n",
    "        with open( in_file, 'r') as datastrings:\n",
    "            for datastring in datastrings:\n",
    "                data = json.loads(datastring)\n",
    "\n",
    "                #Clean utterance and intent\n",
    "                utterance = self.clean_str( data[\"translation\"][\"src\"] )\n",
    "                intent    = self.clean_str( data[\"translation\"][\"tgt\"] )\n",
    "\n",
    "                #Tokenize and append to list\n",
    "                utterances.append( self.to_dict(utterance, include_eos=False))\n",
    "                intents.append( self.to_dict(intent, include_eos=True))\n",
    "\n",
    "        return (utterances, intents)\n",
    "\n",
    "\n",
    "    def write_data(self, dataset):\n",
    "        labelsemantics = self.args.labelsemantics\n",
    "        file_name      = self.args.dataset.split(\"/\")[-1].split(\".\")[0]\n",
    "        with open( f\"{file_name}_{labelsemantics}.json\", \"w\" ) as out_file:\n",
    "            for data in dataset:\n",
    "                data = {\"inputs\": self.tokenizer.decode( data[\"inputs\"] ),\n",
    "                        \"targets\": self.tokenizer.decode( data[\"targets\"] )}\n",
    "                out_file.write( json.dumps( data ) + \"\\n\")\n",
    "\n",
    "datahandler = DataHandler(\n",
    "    punc = (\".\", \"?\", \"!\", \",\", \";\", \":\"),\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor\n",
    "---\n",
    "The below preprocessor implements the different masking strategies we describe in our paper for pretraining. \n",
    "\n",
    "To change the type of masking strategy used, please visit the `args` section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Preprocessor:\n",
    "    \"\"\" Preprocessor class for preprocessing data for pretraining. Responsible for implementing masking strategies. \"\"\"\n",
    "\n",
    "    datahandler: DataHandler\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.utterances, self.intents = self.datahandler.load_data( self.datahandler.args.dataset )\n",
    "        self.ic_package = zip( self.utterances, self.intents )\n",
    "\n",
    "    def label_denoise( self ):\n",
    "        \"\"\"Preprocessing for T5 denoising objective. Returns preprocessed\n",
    "        tokenized and encoded data.\"\"\"\n",
    "        ds = []\n",
    "\n",
    "        for utterance, intent in self.ic_package:\n",
    "            sentinel_id = self.datahandler.tokenizer.convert_tokens_to_ids( \"<extra_id_0>\" )\n",
    "            input = torch.cat(( utterance[\"targets\"], torch.tensor([sentinel_id]) ))\n",
    "            target = torch.cat(( torch.tensor([sentinel_id]), intent[\"targets\"] ))\n",
    "            if input.shape[0] > 512:\n",
    "                input = input[:512]\n",
    "            ds.append( {'inputs': input, 'targets': target} )\n",
    "        return ds\n",
    "    \n",
    "    def intent_classification( self ):\n",
    "        \"\"\"Preprocessing for T5 intent classification objective. Returns preprocessed\n",
    "        tokenized and encoded data.\"\"\"\n",
    "\n",
    "        ds = []\n",
    "        prefix = \"intent classification: \"\n",
    "\n",
    "        for utterance, intent in self.ic_package:\n",
    "            input   = utterance[\"targets\"]\n",
    "            target  = intent[\"targets\"]\n",
    "            if input.shape[0] > 512:\n",
    "                input = input[: 512 - len( prefix )]\n",
    "            ds.append( {'inputs': prefix + input, 'targets': target} )\n",
    "        return ds\n",
    "    \n",
    "    def random_denoising( self ):\n",
    "        pass\n",
    "\n",
    "    def format_pretraining( self ):\n",
    "        pretrain_format = self.datahandler.args.labelsemantics\n",
    "        if pretrain_format == \"label_denoising\":\n",
    "            return self.label_denoise()\n",
    "        elif pretrain_format == \"intent_classification\":\n",
    "            return self.intent_classification()\n",
    "        elif pretrain_format == \"random_denoising\":\n",
    "            return self.random_denoising()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pretraining format\")  \n",
    "        \n",
    "\n",
    "preprocess = Preprocessor(\n",
    "    datahandler = datahandler,\n",
    ")\n",
    "\n",
    "dataset = preprocess.format_pretraining()\n",
    "datahandler.write_data( dataset )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "The below code helps visualize the data, and make sure that the preprocessor is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: If you have a hand only strong in one suit, remain cautious until you can take control.<extra_id_0>\n",
      "target: <extra_id_0> Shoot the Moon in Hearts.</s>\n",
      "\n",
      "input: Dodge the guardian's attacks in editor mode by using the thrust (pressing and holding the Left Trigger) and moving in vertical and horizontal lines once you have passed the boundary of the map.<extra_id_0>\n",
      "target: <extra_id_0> Get the Sandbox Skull in Halo 3.</s>\n",
      "\n",
      "input: Mix resin and hardener according to package directions and pour the solution into a paint tray immediately.<extra_id_0>\n",
      "target: <extra_id_0> Fiberglass a Boat.</s>\n",
      "\n",
      "input: Wait until the fire has spread so the water in the boiler begins to boil, add coal on top of the wood, first in single lumps, and later when pressure has begun to build up, in shovelfuls.<extra_id_0>\n",
      "target: <extra_id_0> Fire a Steam Locomotive.</s>\n",
      "\n",
      "input: Soak the awing in cleaning solution.<extra_id_0>\n",
      "target: <extra_id_0> Remove Mildew from Canvas Awnings.</s>\n",
      "\n",
      "Num examples: 110573\n"
     ]
    }
   ],
   "source": [
    "tokenizer = datahandler.tokenizer\n",
    "\n",
    "for data in dataset[:5]:\n",
    "    print(f'input: {tokenizer.decode(data[\"inputs\"])}\\ntarget: {tokenizer.decode(data[\"targets\"])}\\n')\n",
    "\n",
    "print(\"Num examples:\", len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
