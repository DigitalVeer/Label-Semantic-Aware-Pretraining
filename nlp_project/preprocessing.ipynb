{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "---\n",
    "\n",
    "Here, we preprocess our data, and use 'label denoising' ask our masking strategy for pretraining. The idea is to take an utterance, and a label, and create a \"input\" and \"target\" associated with each one, with the pretraining object being to predict the masked token. For example:\n",
    "\n",
    "```\n",
    "Input: \"I am looking to book a flight from New York to Iceland. <MASK>\"\n",
    "Output: \"<MASK> Book Flight.\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "---\n",
    "We use the Author's settings for the most part, and utilize the same helper functions they did, which come from the TensorFlow Preprocessing Code.\n",
    "\n",
    "* https://github.com/amazon-science/label-aware-pretrain/blob/main/models/preprocessor.py  \n",
    "* https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py\n",
    "\n",
    "The different masking strategies are as follows:\n",
    "\n",
    "* **Label Denoising**: Mask the label, and predict the masked token.\n",
    "* **Intent Classification**: Add a prefix to each unmasked input, and predict the utterance.\n",
    "\n",
    "The args class makes it simple to rerun the preprocessing stage with different settings in this particular notebook. The args class is as follows:\n",
    "\n",
    "```\n",
    "Args(\n",
    "    dataset        # Location of the dataset\n",
    "    seed           # Random seed\n",
    "    labelsemantics # Masking strategy to use\n",
    "    tokenizer      # Tokenizer to use\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    \"\"\"Arguments for pretraining\"\"\"\n",
    "\n",
    "    dataset: str\n",
    "    seed: int\n",
    "    labelsemantics: str\n",
    "    tokenizer: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.dataset.endswith(\".json\")\n",
    "        assert self.labelsemantics in [\"random_denoising\", \"intent_classification\", \"label_denoising\"]\n",
    "\n",
    "args = Args(\n",
    "    dataset         = \"../data/pretraining/dataset/json/train.json\",\n",
    "    seed            = 1248, \n",
    "    labelsemantics  = \"label_denoising\",\n",
    "    tokenizer       = \"t5-base\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handler\n",
    "---\n",
    "Below, we implement our own helper class to handle with reading the raw data from a json and writing it out to a formatted json for pre-training. The below class also helps with tokenization, cleaning up any unsanitized data, and acts as a parent class for the preprocessor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, PreTrainedTokenizerBase\n",
    "import torch, json\n",
    "\n",
    "@dataclass\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    DataHandler class for preprocessing data for pretraining. Responsible for tokenization and writing to file.\n",
    "    \"\"\"\n",
    "\n",
    "    args: Args\n",
    "    punc: tuple \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.tokenizer : PreTrainedTokenizerBase = T5Tokenizer.from_pretrained( self.args.tokenizer )\n",
    "\n",
    "    def to_dict(self, text, include_eos=True):\n",
    "        target = self.tokenizer.encode(text) if include_eos else self.tokenizer.encode(text)[:-1]\n",
    "        return {'inputs': \"\",\n",
    "                'targets': torch.tensor(target)}\n",
    "\n",
    "    def clean_str( self, txt ):\n",
    "        str = txt.strip()\n",
    "        if not str.endswith( self.punc ):\n",
    "            str += \".\"\n",
    "        return str\n",
    "\n",
    "    def load_data(self, in_file):\n",
    "        utterances, intents = [], []\n",
    "        with open( in_file, 'r') as datastrings:\n",
    "            for datastring in datastrings:\n",
    "                data = json.loads(datastring)\n",
    "\n",
    "                #Clean utterance and intent\n",
    "                utterance = self.clean_str( data[\"translation\"][\"src\"] )\n",
    "                intent    = self.clean_str( data[\"translation\"][\"tgt\"] )\n",
    "\n",
    "                #Tokenize and append to list\n",
    "                utterances.append( self.to_dict(utterance, include_eos=False))\n",
    "                intents.append( self.to_dict(intent, include_eos=True))\n",
    "\n",
    "        return (utterances, intents)\n",
    "\n",
    "\n",
    "    def write_data(self, dataset):\n",
    "        with open( self.args.labelsemantics + \".json\", \"w\" ) as out_file:\n",
    "            for data in dataset:\n",
    "                data = {\"inputs\": self.tokenizer.decode( data[\"inputs\"] ),\n",
    "                        \"targets\": self.tokenizer.decode( data[\"targets\"] )}\n",
    "                out_file.write( json.dumps( data ) + \"\\n\")\n",
    "\n",
    "datahandler = DataHandler(\n",
    "    punc = (\".\", \"?\", \"!\", \",\", \";\", \":\"),\n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor\n",
    "---\n",
    "The below preprocessor implements the different masking strategies we describe in our paper for pretraining. \n",
    "\n",
    "To change the type of masking strategy used, please visit the `args` section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Tensor\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid pretraining format\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[0;32m     55\u001b[0m preprocess \u001b[39m=\u001b[39m Preprocessor(\n\u001b[0;32m     56\u001b[0m     datahandler \u001b[39m=\u001b[39m datahandler,\n\u001b[0;32m     57\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m dataset \u001b[39m=\u001b[39m preprocess\u001b[39m.\u001b[39;49mformat_pretraining()\n\u001b[0;32m     60\u001b[0m datahandler\u001b[39m.\u001b[39mwrite_data( dataset )\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36mPreprocessor.format_pretraining\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_denoise()\n\u001b[0;32m     47\u001b[0m \u001b[39melif\u001b[39;00m pretrain_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mintent_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintent_classification()\n\u001b[0;32m     49\u001b[0m \u001b[39melif\u001b[39;00m pretrain_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandom_denoising\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_denoising()\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mPreprocessor.intent_classification\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m512\u001b[39m:\n\u001b[0;32m     36\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[: \u001b[39m512\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m( prefix )]\n\u001b[1;32m---> 37\u001b[0m     ds\u001b[39m.\u001b[39mappend( {\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m: prefix \u001b[39m+\u001b[39;49m \u001b[39minput\u001b[39;49m, \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m: target} )\n\u001b[0;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"Tensor\") to str"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Preprocessor:\n",
    "    \"\"\" Preprocessor class for preprocessing data for pretraining. Responsible for implementing masking strategies. \"\"\"\n",
    "\n",
    "    datahandler: DataHandler\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.utterances, self.intents = self.datahandler.load_data( self.datahandler.args.dataset )\n",
    "        self.ic_package = zip( self.utterances, self.intents )\n",
    "\n",
    "    def label_denoise( self ):\n",
    "        \"\"\"Preprocessing for T5 denoising objective. Returns preprocessed\n",
    "        tokenized and encoded data.\"\"\"\n",
    "        ds = []\n",
    "\n",
    "        for utterance, intent in self.ic_package:\n",
    "            sentinel_id = self.datahandler.tokenizer.convert_tokens_to_ids( \"<extra_id_0>\" )\n",
    "            input = torch.cat(( utterance[\"targets\"], torch.tensor([sentinel_id]) ))\n",
    "            target = torch.cat(( torch.tensor([sentinel_id]), intent[\"targets\"] ))\n",
    "            if input.shape[0] > 512:\n",
    "                input = input[:512]\n",
    "            ds.append( {'inputs': input, 'targets': target} )\n",
    "        return ds\n",
    "    \n",
    "    def intent_classification( self ):\n",
    "        \"\"\"Preprocessing for T5 intent classification objective. Returns preprocessed\n",
    "        tokenized and encoded data.\"\"\"\n",
    "\n",
    "        ds = []\n",
    "        prefix = \"intent classification: \"\n",
    "\n",
    "        for utterance, intent in self.ic_package:\n",
    "            input   = utterance[\"targets\"]\n",
    "            target  = intent[\"targets\"]\n",
    "            if input.shape[0] > 512:\n",
    "                input = input[: 512 - len( prefix )]\n",
    "            ds.append( {'inputs': prefix + input, 'targets': target} )\n",
    "        return ds\n",
    "    \n",
    "    def random_denoising( self ):\n",
    "        pass\n",
    "\n",
    "    def format_pretraining( self ):\n",
    "        pretrain_format = self.datahandler.args.labelsemantics\n",
    "        if pretrain_format == \"label_denoising\":\n",
    "            return self.label_denoise()\n",
    "        elif pretrain_format == \"intent_classification\":\n",
    "            return self.intent_classification()\n",
    "        elif pretrain_format == \"random_denoising\":\n",
    "            return self.random_denoising()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pretraining format\")  \n",
    "        \n",
    "\n",
    "preprocess = Preprocessor(\n",
    "    datahandler = datahandler,\n",
    ")\n",
    "\n",
    "dataset = preprocess.format_pretraining()\n",
    "datahandler.write_data( dataset )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "The below code helps visualize the data, and make sure that the preprocessor is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Choose one person to be “it.” This person is the \"answerer\" and responsible for choosing the objective for each round.<extra_id_0>\n",
      "target: <extra_id_0> Play the \"Mind Reading\" Game.</s>\n",
      "\n",
      "input: Please help me as i am continuously facing the issue in transferring money to my friends, as all my transactions are getting failed.<extra_id_0>\n",
      "target: <extra_id_0> Failed Transfer.</s>\n",
      "\n",
      "input: Your final result: zombies will pile up against the handrail, instead of running around it, leaving them free for pickings.<extra_id_0>\n",
      "target: <extra_id_0> Do the Pile Up Glitch in Tranzit.</s>\n",
      "\n",
      "input: Start every day with self-affirmations.<extra_id_0>\n",
      "target: <extra_id_0> Be Brave.</s>\n",
      "\n",
      "input: Be clear on the type of editing you should do.<extra_id_0>\n",
      "target: <extra_id_0> Be a Good Editor.</s>\n",
      "\n",
      "Num examples: 98924\n"
     ]
    }
   ],
   "source": [
    "tokenizer = datahandler.tokenizer\n",
    "\n",
    "for data in dataset[:5]:\n",
    "    print(f'input: {tokenizer.decode(data[\"inputs\"])}\\ntarget: {tokenizer.decode(data[\"targets\"])}\\n')\n",
    "\n",
    "print(\"Num examples:\", len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
