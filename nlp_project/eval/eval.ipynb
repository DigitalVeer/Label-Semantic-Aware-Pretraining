{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---[Using HuggingFace API]---\n",
      "gold: Book Flight\n",
      "pred: Book Plane.\n",
      "match: 78.49%\n",
      "\n",
      "gold: Book Flight\n",
      "pred: Book Airplane Reservation.\n",
      "match: 74.98%\n",
      "\n",
      "---[Using Local Embeddings]---\n",
      "gold: Book Flight\n",
      "pred: Book Plane.\n",
      "match: 78.49%\n",
      "\n",
      "gold: Book Flight\n",
      "pred: Book Airplane Reservation.\n",
      "match: 74.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "@dataclass\n",
    "class CosineSimilarity:\n",
    "    api_token: str\n",
    "    API_URL: str = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    def __post_init__( self ):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    def headers(self) -> Dict[str, str]:\n",
    "        return {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
    "\n",
    "    def query(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        response = requests.post(self.API_URL, headers=self.headers(), json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    def get_similarity_score(self, gold_intent: str, pred_intent: str) -> float:\n",
    "        data = self.query(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"source_sentence\": gold_intent,\n",
    "                    \"sentences\": [pred_intent]\n",
    "                }\n",
    "            })\n",
    "        return data[0]\n",
    "    \n",
    "    def get_cosine_similarity(self, gold_intent: str, pred_intent: str) -> float:\n",
    "\n",
    "        #Compute embedding for both lists\n",
    "        embedding_1 = self.model.encode( gold_intent, convert_to_tensor=True)\n",
    "        embedding_2 = self.model.encode( pred_intent, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(embedding_1, embedding_2).item()\n",
    "        return sim\n",
    "\n",
    "\n",
    "    def compare(self, gold_intent: str, pred_intent: str) -> None:\n",
    "        cosine_sim = round(self.get_similarity_score(gold_intent, pred_intent) * 100, 2)\n",
    "        print(f\"gold: {gold_intent}\\npred: {pred_intent}\\nmatch: {cosine_sim}%\\n\")\n",
    "\n",
    "    def compare_embed(self, gold_intent: str, pred_intent: str) -> None:\n",
    "        cosine_sim = round(self.get_cosine_similarity(gold_intent, pred_intent) * 100, 2)\n",
    "        print(f\"gold: {gold_intent}\\npred: {pred_intent}\\nmatch: {cosine_sim}%\\n\")\n",
    "\n",
    "print(\"---[Using HuggingFace API]---\")\n",
    "\n",
    "#Cosine Similarity Example\n",
    "similarity_checker = CosineSimilarity( api_token=\"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\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Plane.\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Airplane Reservation.\" )\n",
    "\n",
    "print(\"---[Using Local Embeddings]---\")\n",
    "\n",
    "#Embedding Similarity Example\n",
    "similarity_checker.compare_embed( \"Book Flight\", \"Book Plane.\" )\n",
    "similarity_checker.compare_embed( \"Book Flight\", \"Book Airplane Reservation.\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetricsDemo:\n",
    "  pred_file: str\n",
    "  gold_file: str\n",
    "  embed_handler: CosineSimilarity \n",
    "\n",
    "  def is_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "        return gold_intent == pred_intent\n",
    "    \n",
    "  def first_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "     return gold_intent.split()[0] == pred_intent.split()[0]\n",
    "\n",
    "  def exist(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "    return len( pred_intent ) > 0 and len( gold_intent ) > 0\n",
    "\n",
    "  def calculate_accuracy(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "      pred_lines = pred_f.readlines()\n",
    "      gold_lines = gold_f.readlines()\n",
    "      \n",
    "      assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    first_word_correct: float = 0.0\n",
    "    exact_match: float = 0.0\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "\n",
    "        pred_intent = pred_line.strip()\n",
    "    \n",
    "        total += 1.0\n",
    "        if self.first_match(gold_intent, pred_intent):\n",
    "            first_word_correct += 1.0\n",
    "        if self.exist( gold_intent, pred_intent ) and self.is_match( gold_intent, pred_intent ):\n",
    "            exact_match += 1.0\n",
    "\n",
    "    first_word_correct = round( first_word_correct / total * 100, 2 )\n",
    "    exact_match = round( exact_match / total * 100, 2 )\n",
    "\n",
    "    return first_word_correct, exact_match\n",
    "\n",
    "  def calculate_bleu_score(self) -> None:\n",
    "    smoothie = SmoothingFunction().method1 \n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    blue_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        reference = [gold_intent.split()]\n",
    "        hypothesis = pred_intent.split()\n",
    "        blue_scores.append(sentence_bleu(reference, hypothesis, smoothing_function=smoothie))\n",
    "\n",
    "    blue_score = sum(blue_scores) / total * 100\n",
    "    \n",
    "    return blue_score\n",
    "  \n",
    "  def jaccard_similarity(self, label1: str, label2: str) -> float:\n",
    "    # Tokenize the intent labels\n",
    "    tokens1 = set(label1.split())\n",
    "    tokens2 = set(label2.split())\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    similarity = intersection / union if union != 0 else 0.0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "  def calculate_jaccard_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    jaccard_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        jaccard_scores.append( self.jaccard_similarity( gold_intent, pred_intent ) )\n",
    "\n",
    "    jaccard_score = sum(jaccard_scores) / total * 100\n",
    "    \n",
    "    return jaccard_score\n",
    "  \n",
    "  def cosine_similarity(self, label1: str, label2: str) -> float:\n",
    "    return self.embed_handler.get_cosine_similarity( label1, label2 )\n",
    "\n",
    "  def calculate_cosine_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    cosine_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        cosine_scores.append( self.cosine_similarity( gold_intent, pred_intent ) )\n",
    "\n",
    "    return cosine_scores\n",
    "    \n",
    "\n",
    "  def evaluate(self) -> dict:\n",
    "    accuracy = self.calculate_accuracy()\n",
    "    bleu_score = self.calculate_bleu_score()\n",
    "    jaccard_score = self.calculate_jaccard_similarity()\n",
    "    cosine_scores = self.calculate_cosine_similarity()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy':  {\n",
    "            'first_word':  accuracy[0],\n",
    "            'exact_match': accuracy[1]\n",
    "        },\n",
    "        'bleu_score': bleu_score,\n",
    "        'jaccard_score': jaccard_score,\n",
    "        'cosine_similarity': cosine_scores\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'first_word': 98.86, 'exact_match': 98.86},\n",
       " 'bleu_score': 17.57956216781354,\n",
       " 'jaccard_score': 98.85714285714286,\n",
       " 'cosine_similarity': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.17485783994197845,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  0.17485783994197845,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0000001192092896,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  0.9999999403953552,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.4406600594520569,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = EvaluationMetricsDemo( \n",
    "    embed_handler=similarity_checker,\n",
    "    gold_file=\"results/Labels_gold_silver_model_3_1_full_resource.txt\",\n",
    "    pred_file=\"results/Preds_gold_silver_model_3_1_full_resource.txt\" )\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Word Accuracy: 98.86%\n",
      "Exact Match Accuracy: 98.86%\n"
     ]
    }
   ],
   "source": [
    "first_word_correct, exact_match = metrics.calculate_accuracy()\n",
    "print( f\"First Word Accuracy: {first_word_correct}%\" )\n",
    "print( f\"Exact Match Accuracy: {exact_match}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 17.57956216781354%\n"
     ]
    }
   ],
   "source": [
    "blue_score = metrics.calculate_bleu_score()\n",
    "print( f\"BLEU Score: {blue_score}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Jaccard Similarity: 98.85714285714286%\n"
     ]
    }
   ],
   "source": [
    "avg_jaccard = metrics.calculate_jaccard_similarity()\n",
    "print( f\"Average Jaccard Similarity: {avg_jaccard}%\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Across All Files\n",
    "---\n",
    "Check accuracy across all n-shot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Labels_gold_silver_model_3_1_eight_shot.txt and Preds_gold_silver_model_3_1_eight_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_four_shot.txt and Preds_gold_silver_model_3_1_four_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_full_resource.txt and Preds_gold_silver_model_3_1_full_resource.txt:\n",
      "{'accuracy': {'first_word': 98.86, 'exact_match': 98.86}, 'bleu_score': 17.57956216781354, 'jaccard_score': 98.85714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_one_shot.txt and Preds_gold_silver_model_3_1_one_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_sixteen_shot.txt and Preds_gold_silver_model_3_1_sixteen_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_two_shot.txt and Preds_gold_silver_model_3_1_two_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_results( path: str ) -> dict:\n",
    "    all_files = os.listdir( path )\n",
    "    labels = [ file for file in all_files if file.startswith( \"Labels\" ) ]\n",
    "    preds  = [ file.replace( \"Labels\", \"Preds\" ) for file in labels ]\n",
    "    for label, pred in zip( labels, preds ):\n",
    "        metrics = EvaluationMetricsDemo( gold_file=f\"{path}/{label}\", pred_file=f\"{path}/{pred}\" )\n",
    "        results = metrics.evaluate()\n",
    "        print( f\"Results for {label} and {pred}:\" )\n",
    "        print( results )\n",
    "        print( \"\\n\" )\n",
    "        \n",
    "results = get_results( \"results\" )\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
