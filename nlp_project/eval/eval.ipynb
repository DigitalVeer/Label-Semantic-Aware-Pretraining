{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---[Using HuggingFace API]---\n",
      "gold: Book Flight\n",
      "pred: Book Plane.\n",
      "match: 78.49%\n",
      "\n",
      "gold: Book Flight\n",
      "pred: Book Airplane Reservation.\n",
      "match: 74.98%\n",
      "\n",
      "---[Using Local Embeddings]---\n",
      "gold: Book Flight\n",
      "pred: Book Plane.\n",
      "match: 78.49%\n",
      "\n",
      "gold: Book Flight\n",
      "pred: Book Airplane Reservation.\n",
      "match: 74.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "@dataclass\n",
    "class CosineSimilarity:\n",
    "    api_token: str\n",
    "    API_URL: str = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    def __post_init__( self ):\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    def headers(self) -> Dict[str, str]:\n",
    "        return {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
    "\n",
    "    def query(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        response = requests.post(self.API_URL, headers=self.headers(), json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    def get_similarity_score(self, gold_intent: str, pred_intent: str) -> float:\n",
    "        data = self.query(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"source_sentence\": gold_intent,\n",
    "                    \"sentences\": [pred_intent]\n",
    "                }\n",
    "            })\n",
    "        return data[0]\n",
    "    \n",
    "    def get_cosine_similarity(self, gold_intent: str, pred_intent: str) -> float:\n",
    "\n",
    "        #Compute embedding for both lists\n",
    "        embedding_1 = self.model.encode( gold_intent, convert_to_tensor=True)\n",
    "        embedding_2 = self.model.encode( pred_intent, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(embedding_1, embedding_2).item()\n",
    "        return sim\n",
    "\n",
    "\n",
    "    def compare(self, gold_intent: str, pred_intent: str) -> None:\n",
    "        cosine_sim = round(self.get_similarity_score(gold_intent, pred_intent) * 100, 2)\n",
    "        print(f\"gold: {gold_intent}\\npred: {pred_intent}\\nmatch: {cosine_sim}%\\n\")\n",
    "\n",
    "    def compare_embed(self, gold_intent: str, pred_intent: str) -> None:\n",
    "        cosine_sim = round(self.get_cosine_similarity(gold_intent, pred_intent) * 100, 2)\n",
    "        print(f\"gold: {gold_intent}\\npred: {pred_intent}\\nmatch: {cosine_sim}%\\n\")\n",
    "\n",
    "print(\"---[Using HuggingFace API]---\")\n",
    "\n",
    "#Cosine Similarity Example\n",
    "similarity_checker = CosineSimilarity( api_token=\"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\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Plane.\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Airplane Reservation.\" )\n",
    "\n",
    "print(\"---[Using Local Embeddings]---\")\n",
    "\n",
    "#Embedding Similarity Example\n",
    "similarity_checker.compare_embed( \"Book Flight\", \"Book Plane.\" )\n",
    "similarity_checker.compare_embed( \"Book Flight\", \"Book Airplane Reservation.\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetricsDemo:\n",
    "  pred_file: str\n",
    "  gold_file: str\n",
    "  embed_handler: CosineSimilarity \n",
    "\n",
    "  def is_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "        return gold_intent == pred_intent\n",
    "    \n",
    "  def first_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "     return gold_intent.split()[0] == pred_intent.split()[0]\n",
    "\n",
    "  def exist(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "    return len( pred_intent ) > 0 and len( gold_intent ) > 0\n",
    "\n",
    "  def calculate_accuracy(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "      pred_lines = pred_f.readlines()\n",
    "      gold_lines = gold_f.readlines()\n",
    "      \n",
    "      assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    first_word_correct: float = 0.0\n",
    "    exact_match: float = 0.0\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "\n",
    "        pred_intent = pred_line.strip()\n",
    "    \n",
    "        total += 1.0\n",
    "        if self.first_match(gold_intent, pred_intent):\n",
    "            first_word_correct += 1.0\n",
    "        if self.exist( gold_intent, pred_intent ) and self.is_match( gold_intent, pred_intent ):\n",
    "            exact_match += 1.0\n",
    "\n",
    "    first_word_correct = round( first_word_correct / total * 100, 2 )\n",
    "    exact_match = round( exact_match / total * 100, 2 )\n",
    "\n",
    "    return first_word_correct, exact_match\n",
    "\n",
    "  def calculate_bleu_score(self) -> None:\n",
    "    smoothie = SmoothingFunction().method1 \n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    blue_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        reference = [gold_intent.split()]\n",
    "        hypothesis = pred_intent.split()\n",
    "        blue_scores.append(sentence_bleu(reference, hypothesis, smoothing_function=smoothie))\n",
    "\n",
    "    blue_score = sum(blue_scores) / total * 100\n",
    "    \n",
    "    return blue_score\n",
    "  \n",
    "  def jaccard_similarity(self, label1: str, label2: str) -> float:\n",
    "    # Tokenize the intent labels\n",
    "    tokens1 = set(label1.split())\n",
    "    tokens2 = set(label2.split())\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    similarity = intersection / union if union != 0 else 0.0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "  def calculate_jaccard_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    jaccard_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        jaccard_scores.append( self.jaccard_similarity( gold_intent, pred_intent ) )\n",
    "\n",
    "    jaccard_score = sum(jaccard_scores) / total * 100\n",
    "    \n",
    "    return jaccard_score\n",
    "  \n",
    "  def cosine_similarity(self, label1: str, label2: str) -> float:\n",
    "    return self.embed_handler.get_cosine_similarity( label1, label2 )\n",
    "\n",
    "  def calculate_cosine_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    cosine_scores: int = 0\n",
    "    total: float = 0.0\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "        total += 1.0\n",
    "\n",
    "        score = self.cosine_similarity( gold_intent, pred_intent )\n",
    "        if score >= 0.70:\n",
    "            cosine_scores += 1\n",
    "\n",
    "    return cosine_scores / total\n",
    "    \n",
    "\n",
    "  def evaluate(self) -> dict:\n",
    "    accuracy = self.calculate_accuracy()\n",
    "    bleu_score = self.calculate_bleu_score()\n",
    "    jaccard_score = self.calculate_jaccard_similarity()\n",
    "    cosine_scores = self.calculate_cosine_similarity()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy':  {\n",
    "            'first_word':  accuracy[0],\n",
    "            'exact_match': accuracy[1]\n",
    "        },\n",
    "        'bleu_score': bleu_score,\n",
    "        'jaccard_score': jaccard_score,\n",
    "        'cosine_similarity': cosine_scores\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'first_word': 98.86, 'exact_match': 98.86},\n",
       " 'bleu_score': 17.57956216781354,\n",
       " 'jaccard_score': 98.85714285714286,\n",
       " 'cosine_similarity': 0.9885714285714285}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = EvaluationMetricsDemo( \n",
    "    embed_handler=similarity_checker,\n",
    "    gold_file=\"results/Labels_gold_silver_model_3_1_full_resource.txt\",\n",
    "    pred_file=\"results/Preds_gold_silver_model_3_1_full_resource.txt\" )\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Word Accuracy: 98.86%\n",
      "Exact Match Accuracy: 98.86%\n"
     ]
    }
   ],
   "source": [
    "first_word_correct, exact_match = metrics.calculate_accuracy()\n",
    "print( f\"First Word Accuracy: {first_word_correct}%\" )\n",
    "print( f\"Exact Match Accuracy: {exact_match}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 17.57956216781354%\n"
     ]
    }
   ],
   "source": [
    "blue_score = metrics.calculate_bleu_score()\n",
    "print( f\"BLEU Score: {blue_score}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Jaccard Similarity: 98.85714285714286%\n"
     ]
    }
   ],
   "source": [
    "avg_jaccard = metrics.calculate_jaccard_similarity()\n",
    "print( f\"Average Jaccard Similarity: {avg_jaccard}%\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Across All Files\n",
    "---\n",
    "Check accuracy across all n-shot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Labels_gold_silver_model_2_eight_shot.txt and Preds_gold_silver_model_2_eight_shot.txt:\n",
      "{'accuracy': {'first_word': 90.71, 'exact_match': 88.86}, 'bleu_score': 36.46701448101957, 'jaccard_score': 89.22857142857151, 'cosine_similarity': 0.8885714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_2_four_shot.txt and Preds_gold_silver_model_2_four_shot.txt:\n",
      "{'accuracy': {'first_word': 88.71, 'exact_match': 82.43}, 'bleu_score': 35.06086900567161, 'jaccard_score': 83.74047619047613, 'cosine_similarity': 0.8242857142857143}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_2_full_resource.txt and Preds_gold_silver_model_2_full_resource.txt:\n",
      "{'accuracy': {'first_word': 98.0, 'exact_match': 96.71}, 'bleu_score': 40.82048378657413, 'jaccard_score': 96.97142857142863, 'cosine_similarity': 0.9671428571428572}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_2_one_shot.txt and Preds_gold_silver_model_2_one_shot.txt:\n",
      "{'accuracy': {'first_word': 61.29, 'exact_match': 54.0}, 'bleu_score': 25.68582631106849, 'jaccard_score': 57.04523809523801, 'cosine_similarity': 0.5485714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_2_sixteen_shot.txt and Preds_gold_silver_model_2_sixteen_shot.txt:\n",
      "{'accuracy': {'first_word': 97.29, 'exact_match': 94.71}, 'bleu_score': 39.80672740342876, 'jaccard_score': 95.22857142857153, 'cosine_similarity': 0.9471428571428572}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_2_two_shot.txt and Preds_gold_silver_model_2_two_shot.txt:\n",
      "{'accuracy': {'first_word': 85.57, 'exact_match': 74.14}, 'bleu_score': 31.78664331601891, 'jaccard_score': 76.46666666666654, 'cosine_similarity': 0.7414285714285714}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_eight_shot.txt and Preds_gold_silver_model_3_1_eight_shot.txt:\n",
      "{'accuracy': {'first_word': 95.29, 'exact_match': 91.86}, 'bleu_score': 38.57870077335184, 'jaccard_score': 92.54285714285729, 'cosine_similarity': 0.9185714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_four_shot.txt and Preds_gold_silver_model_3_1_four_shot.txt:\n",
      "{'accuracy': {'first_word': 85.0, 'exact_match': 81.71}, 'bleu_score': 33.89011101184944, 'jaccard_score': 82.82857142857152, 'cosine_similarity': 0.8257142857142857}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_full_resource.txt and Preds_gold_silver_model_3_1_full_resource.txt:\n",
      "{'accuracy': {'first_word': 98.57, 'exact_match': 96.71}, 'bleu_score': 40.77993336786933, 'jaccard_score': 97.08571428571437, 'cosine_similarity': 0.9671428571428572}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_one_shot.txt and Preds_gold_silver_model_3_1_one_shot.txt:\n",
      "{'accuracy': {'first_word': 42.0, 'exact_match': 26.71}, 'bleu_score': 13.73965731926588, 'jaccard_score': 31.526190476190465, 'cosine_similarity': 0.2757142857142857}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_sixteen_shot.txt and Preds_gold_silver_model_3_1_sixteen_shot.txt:\n",
      "{'accuracy': {'first_word': 96.57, 'exact_match': 93.86}, 'bleu_score': 39.516747657910486, 'jaccard_score': 94.40000000000013, 'cosine_similarity': 0.9385714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_two_shot.txt and Preds_gold_silver_model_3_1_two_shot.txt:\n",
      "{'accuracy': {'first_word': 67.0, 'exact_match': 48.0}, 'bleu_score': 22.09986315378374, 'jaccard_score': 56.97857142857136, 'cosine_similarity': 0.5}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_results( path: str ) -> dict:\n",
    "    all_files = os.listdir( path )\n",
    "    labels = [ file for file in all_files if file.startswith( \"Labels\" ) ]\n",
    "    preds  = [ file.replace( \"Labels\", \"Preds\" ) for file in labels ]\n",
    "    for label, pred in zip( labels, preds ):\n",
    "        metrics = EvaluationMetricsDemo(\n",
    "                embed_handler=similarity_checker,\n",
    "                gold_file=f\"{path}/{label}\",\n",
    "                pred_file=f\"{path}/{pred}\"\n",
    "        )\n",
    "        results = metrics.evaluate()\n",
    "        print( f\"Results for {label} and {pred}:\" )\n",
    "        print( results )\n",
    "        print( \"\\n\" )\n",
    "        \n",
    "#get all folders under SNIPS\n",
    "folders = os.listdir( \"SNIPS\" )\n",
    "for folder in folders:\n",
    "    get_results( f\"SNIPS/{folder}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rate Book.', 'Search Creative Work.', 'Search Screening Event.', 'Get Weather.', 'Play Music.', 'Add To Playlist.', 'Book Restaurant.'} {'Rate Book.', 'Search Creative Work.', 'Search Screening Event.', 'Get Weather.', 'ε', 'Play Music.', 'Add To Playlist.', 'Book Restaurant.'}\n",
      "Precisions:  [1.         1.         0.984375   0.66666667 1.         0.37219731\n",
      " 0.72881356 0.00625   ]\n",
      "Recalls:  [0.89 0.62 0.63 0.12 0.26 0.83 0.43 1.  ]\n",
      "F1 scores:  [0.94179894 0.7654321  0.76829268 0.20338983 0.41269841 0.51393189\n",
      " 0.5408805  0.01242236]\n",
      "<function confusion_matrix at 0x000002497EF22320>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m fig \u001b[39m=\u001b[39m ff\u001b[39m.\u001b[39mcreate_annotated_heatmap(df_cm\u001b[39m.\u001b[39mto_numpy(), colorscale\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBlues\u001b[39m\u001b[39m'\u001b[39m, x\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(df_cm\u001b[39m.\u001b[39mcolumns), y\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(df_cm\u001b[39m.\u001b[39mindex))\n\u001b[0;32m     71\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(height\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, title_text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<b>Confusion Matrix</b>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m fig\u001b[39m.\u001b[39;49mwrite_image(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(out_path, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mplotly_conf_mat_\u001b[39;49m\u001b[39m{\u001b[39;49;00mconfmat_name\u001b[39m}\u001b[39;49;00m\u001b[39m_1shot.png\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     74\u001b[0m \u001b[39m# normalized figure\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df_cmn \u001b[39m=\u001b[39m df_cm\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39marray(df_cm\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))[:, np\u001b[39m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\plotly\\basedatatypes.py:3821\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3761\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m \u001b[39mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[0;32m   3763\u001b[0m \u001b[39mobject\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3817\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3818\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3819\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3821\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39mwrite_image(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\plotly\\io\\_kaleido.py:267\u001b[0m, in \u001b[0;36mwrite_image\u001b[1;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m                 )\n\u001b[0;32m    262\u001b[0m             )\n\u001b[0;32m    264\u001b[0m     \u001b[39m# Request image\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[39m# -------------\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[39m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     img_data \u001b[39m=\u001b[39m to_image(\n\u001b[0;32m    268\u001b[0m         fig,\n\u001b[0;32m    269\u001b[0m         \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m    270\u001b[0m         scale\u001b[39m=\u001b[39;49mscale,\n\u001b[0;32m    271\u001b[0m         width\u001b[39m=\u001b[39;49mwidth,\n\u001b[0;32m    272\u001b[0m         height\u001b[39m=\u001b[39;49mheight,\n\u001b[0;32m    273\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    274\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    275\u001b[0m     )\n\u001b[0;32m    277\u001b[0m     \u001b[39m# Open file\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[39m# ---------\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m         \u001b[39m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         \u001b[39m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\plotly\\io\\_kaleido.py:133\u001b[0m, in \u001b[0;36mto_image\u001b[1;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[39m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m scope \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mwhich can be installed using pip:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    $ pip install -U kaleido\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         )\n\u001b[0;32m    141\u001b[0m     \u001b[39m# Validate figure\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# ---------------\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     fig_dict \u001b[39m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[1;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd \n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "def load_intents_json(intents_file):\n",
    "    intents = []\n",
    "    with open(intents_file, 'r') as examples:\n",
    "        for example in examples:\n",
    "            data = json.loads(example)\n",
    "            intent = data[\"translation\"][\"tgt\"]\n",
    "            intents.append(intent)\n",
    "    return intents\n",
    "\n",
    "def load_intents_list(intents_file):\n",
    "    intents = []\n",
    "    with open(intents_file, 'r') as intent_preds:\n",
    "        for intent in intent_preds:\n",
    "            intents.append(intent.strip())\n",
    "    return intents\n",
    "\n",
    "\n",
    "confmat_name = \"snips\"\n",
    "\n",
    "true_intents = load_intents_list(\"SNIPS/model_2 results/Labels_gold_silver_model_2_one_shot.txt\")\n",
    "pred_intents = load_intents_list(\"SNIPS/model_2 results/Preds_gold_silver_model_2_one_shot.txt\")\n",
    "\n",
    "\n",
    "# replace all bad generated intents with BAD GENERATION\n",
    "intent_set = set(true_intents)\n",
    "for idx, pred in enumerate(pred_intents):\n",
    "    if pred not in intent_set:\n",
    "        pred_intents[idx] = \"ε\"\n",
    "\n",
    "# FOR DEBUGGING\n",
    "print(set(true_intents), set(pred_intents))\n",
    "for intent in set(true_intents):\n",
    "    if intent not in set(pred_intents):\n",
    "        pred_intents.append(intent)\n",
    "        true_intents.append(\"ε\")\n",
    "\n",
    "if \"ε\" not in set(true_intents):\n",
    "    true_intents.append(\"ε\")\n",
    "    pred_intents.append(\"ε\")\n",
    "\n",
    "precisions = defaultdict(list)\n",
    "recalls = defaultdict(list)\n",
    "f1s = defaultdict(list)\n",
    "precs, recs, f1s, supports = precision_recall_fscore_support(true_intents, pred_intents)\n",
    "\n",
    "print(\"Precisions: \", precs)\n",
    "print(\"Recalls: \", recs)\n",
    "print(\"F1 scores: \", f1s)\n",
    "\n",
    "out_path = \"confmat_figures\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "data = confusion_matrix(true_intents, pred_intents)\n",
    "print(confusion_matrix)\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(pred_intents), index=np.unique(true_intents))\n",
    "df_cm.index.name = \"True Intent\"\n",
    "df_cm.columns.name = \"Predicted Intent\"\n",
    "\n",
    "# non-normalized figure\n",
    "fig = ff.create_annotated_heatmap(df_cm.to_numpy(), colorscale='Blues', x=list(df_cm.columns), y=list(df_cm.index))\n",
    "fig.update_layout(height=600, width=800, title_text=\"<b>Confusion Matrix</b>\")\n",
    "fig.show()\n",
    "\n",
    "# normalized figure\n",
    "df_cmn = df_cm.astype('float') / np.array(df_cm.sum(axis=1))[:, np.newaxis]\n",
    "\n",
    "df_cmn.index.name = \"True Intent\"\n",
    "df_cmn.columns.name = \"Predicted Intent\"\n",
    "fig = ff.create_annotated_heatmap(df_cmn.to_numpy(), colorscale='Blues', x=list(df_cmn.columns), y=list(df_cmn.index))\n",
    "fig.update_layout(height=600, width=800, title_text=\"<b>Normalized Confusion Matrix</b>\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
