{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold: Book Flight\n",
      "pred: Book Plane.\n",
      "match: 78.49%\n",
      "\n",
      "gold: Book Flight\n",
      "pred: Book Airplane Reservation.\n",
      "match: 74.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "@dataclass\n",
    "class CosineSimilarity:\n",
    "    api_token: str\n",
    "    API_URL: str = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    def headers(self) -> Dict[str, str]:\n",
    "        return {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
    "\n",
    "    def query(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        response = requests.post(self.API_URL, headers=self.headers(), json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    def get_similarity_score(self, gold_intent: str, pred_intent: str) -> float:\n",
    "        data = self.query(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"source_sentence\": gold_intent,\n",
    "                    \"sentences\": [pred_intent]\n",
    "                }\n",
    "            })\n",
    "        return data[0]\n",
    "\n",
    "    def compare(self, gold_intent: str, pred_intent: str) -> None:\n",
    "        cosine_sim = round(self.get_similarity_score(gold_intent, pred_intent) * 100, 2)\n",
    "        print(f\"gold: {gold_intent}\\npred: {pred_intent}\\nmatch: {cosine_sim}%\\n\")\n",
    "\n",
    "#Cosine Similarity Example\n",
    "similarity_checker = CosineSimilarity( api_token=\"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\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Plane.\" )\n",
    "similarity_checker.compare( \"Book Flight\", \"Book Airplane Reservation.\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetricsDemo:\n",
    "  pred_file: str\n",
    "  gold_file: str\n",
    "\n",
    "  def is_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "        return gold_intent == pred_intent\n",
    "    \n",
    "  def first_match(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "     return gold_intent.split()[0] == pred_intent.split()[0]\n",
    "\n",
    "  def exist(self, gold_intent: str, pred_intent: list) -> bool:\n",
    "    return len( pred_intent ) > 0 and len( gold_intent ) > 0\n",
    "\n",
    "  def calculate_accuracy(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "      pred_lines = pred_f.readlines()\n",
    "      gold_lines = gold_f.readlines()\n",
    "      \n",
    "      assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    first_word_correct: float = 0.0\n",
    "    exact_match: float = 0.0\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "\n",
    "        pred_intent = pred_line.strip()\n",
    "    \n",
    "        total += 1.0\n",
    "        if self.first_match(gold_intent, pred_intent):\n",
    "            first_word_correct += 1.0\n",
    "        if self.exist( gold_intent, pred_intent ) and self.is_match( gold_intent, pred_intent ):\n",
    "            exact_match += 1.0\n",
    "\n",
    "    first_word_correct = round( first_word_correct / total * 100, 2 )\n",
    "    exact_match = round( exact_match / total * 100, 2 )\n",
    "\n",
    "    return first_word_correct, exact_match\n",
    "\n",
    "  def calculate_bleu_score(self) -> None:\n",
    "    smoothie = SmoothingFunction().method1 \n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    blue_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        reference = [gold_intent.split()]\n",
    "        hypothesis = pred_intent.split()\n",
    "        blue_scores.append(sentence_bleu(reference, hypothesis, smoothing_function=smoothie))\n",
    "\n",
    "    blue_score = sum(blue_scores) / total * 100\n",
    "    \n",
    "    return blue_score\n",
    "  \n",
    "  def jaccard_similarity(self, label1: str, label2: str) -> float:\n",
    "    # Tokenize the intent labels\n",
    "    tokens1 = set(label1.split())\n",
    "    tokens2 = set(label2.split())\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    similarity = intersection / union if union != 0 else 0.0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "  def calculate_jaccard_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    total: float = 0.0\n",
    "    jaccard_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        total += 1.0\n",
    "        jaccard_scores.append( self.jaccard_similarity( gold_intent, pred_intent ) )\n",
    "\n",
    "    jaccard_score = sum(jaccard_scores) / total * 100\n",
    "    \n",
    "    return jaccard_score\n",
    "  \n",
    "  def cosine_similarity(self, label1: str, label2: str) -> float:\n",
    "    similarity = CosineSimilarity( api_token=\"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\" )\n",
    "    return similarity.get_similarity_score( label1, label2 )\n",
    "\n",
    "  def calculate_cosine_similarity(self) -> None:\n",
    "    with open(self.pred_file, \"r\") as pred_f, open(self.gold_file, \"r\") as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "\n",
    "        assert len(pred_lines) == len(gold_lines)\n",
    "\n",
    "    cosine_scores: list = []\n",
    "\n",
    "    for pred_line, gold_line in zip(pred_lines, gold_lines):\n",
    "        if self.gold_file.endswith(\"json\"):\n",
    "            gold_intent = json.loads(gold_line)[\"translation\"][\"tgt\"]\n",
    "        else:\n",
    "            gold_intent = gold_line.strip()\n",
    "        pred_intent = pred_line.strip()\n",
    "\n",
    "        cosine_scores.append( self.cosine_similarity( gold_intent, pred_intent ) )\n",
    "\n",
    "    return cosine_scores\n",
    "    \n",
    "\n",
    "  def evaluate(self) -> dict:\n",
    "    accuracy = self.calculate_accuracy()\n",
    "    bleu_score = self.calculate_bleu_score()\n",
    "    jaccard_score = self.calculate_jaccard_similarity()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy':  {\n",
    "            'first_word':  accuracy[0],\n",
    "            'exact_match': accuracy[1]\n",
    "        },\n",
    "        'bleu_score': bleu_score,\n",
    "        'jaccard_score': jaccard_score,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'first_word': 98.86, 'exact_match': 98.86},\n",
       " 'bleu_score': 17.57956216781354,\n",
       " 'jaccard_score': 98.85714285714286}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = EvaluationMetricsDemo( \n",
    "    gold_file=\"results/Labels_gold_silver_model_3_1_full_resource.txt\",\n",
    "    pred_file=\"results/Preds_gold_silver_model_3_1_full_resource.txt\" )\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Word Accuracy: 98.86%\n",
      "Exact Match Accuracy: 98.86%\n"
     ]
    }
   ],
   "source": [
    "first_word_correct, exact_match = metrics.calculate_accuracy()\n",
    "print( f\"First Word Accuracy: {first_word_correct}%\" )\n",
    "print( f\"Exact Match Accuracy: {exact_match}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 17.57956216781354%\n"
     ]
    }
   ],
   "source": [
    "blue_score = metrics.calculate_bleu_score()\n",
    "print( f\"BLEU Score: {blue_score}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Jaccard Similarity: 98.85714285714286%\n"
     ]
    }
   ],
   "source": [
    "avg_jaccard = metrics.calculate_jaccard_similarity()\n",
    "print( f\"Average Jaccard Similarity: {avg_jaccard}%\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Across All Files\n",
    "---\n",
    "Check accuracy across all n-shot settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Labels_gold_silver_model_3_1_eight_shot.txt and Preds_gold_silver_model_3_1_eight_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_four_shot.txt and Preds_gold_silver_model_3_1_four_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_full_resource.txt and Preds_gold_silver_model_3_1_full_resource.txt:\n",
      "{'accuracy': {'first_word': 98.86, 'exact_match': 98.86}, 'bleu_score': 17.57956216781354, 'jaccard_score': 98.85714285714286}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_one_shot.txt and Preds_gold_silver_model_3_1_one_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_sixteen_shot.txt and Preds_gold_silver_model_3_1_sixteen_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n",
      "Results for Labels_gold_silver_model_3_1_two_shot.txt and Preds_gold_silver_model_3_1_two_shot.txt:\n",
      "{'accuracy': {'first_word': 0.0, 'exact_match': 0.0}, 'bleu_score': 0.0, 'jaccard_score': 0.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_results( path: str ) -> dict:\n",
    "    all_files = os.listdir( path )\n",
    "    labels = [ file for file in all_files if file.startswith( \"Labels\" ) ]\n",
    "    preds  = [ file.replace( \"Labels\", \"Preds\" ) for file in labels ]\n",
    "    for label, pred in zip( labels, preds ):\n",
    "        metrics = EvaluationMetricsDemo( gold_file=f\"{path}/{label}\", pred_file=f\"{path}/{pred}\" )\n",
    "        results = metrics.evaluate()\n",
    "        print( f\"Results for {label} and {pred}:\" )\n",
    "        print( results )\n",
    "        print( \"\\n\" )\n",
    "        \n",
    "results = get_results( \"results\" )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like distilbert-base-nli-mean-tokens is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1291\u001b[0m         \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1292\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConnection error, and we cannot find the requested files in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m the disk cache. Please try again or make sure your Internet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m connection is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[39m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m word1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapple\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m word2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morange\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m similarity_score \u001b[39m=\u001b[39m cosine_similarity(word1, word2)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe cosine similarity between \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword1\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword2\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is: \u001b[39m\u001b[39m{\u001b[39;00msimilarity_score\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(word1, word2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcosine_similarity\u001b[39m(word1, word2):\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Load pre-trained model and tokenizer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-nli-mean-tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      9\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Encode the input words\u001b[39;00m\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:657\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mif\u001b[39;00m config_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 657\u001b[0m         config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    658\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    659\u001b[0m         )\n\u001b[0;32m    660\u001b[0m     config_tokenizer_class \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mtokenizer_class\n\u001b[0;32m    661\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoTokenizer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:916\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    915\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 916\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    918\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    572\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    575\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    630\u001b[0m         configuration_file,\n\u001b[0;32m    631\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    632\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    633\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    634\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    635\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    636\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    637\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    638\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    639\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    640\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    641\u001b[0m     )\n\u001b[0;32m    642\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:443\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     )\n\u001b[0;32m    449\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like distilbert-base-nli-mean-tokens is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity(word1, word2):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = \"distilbert-base-nli-mean-tokens\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the input words\n",
    "    encoded_input = tokenizer([word1, word2], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Obtain the embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    embeddings = model_output.last_hidden_state\n",
    "    similarity_score = cosine_similarity(embeddings[0], embeddings[1])[0][0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage\n",
    "word1 = \"apple\"\n",
    "word2 = \"orange\"\n",
    "similarity_score = cosine_similarity(word1, word2)\n",
    "print(f\"The cosine similarity between '{word1}' and '{word2}' is: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
