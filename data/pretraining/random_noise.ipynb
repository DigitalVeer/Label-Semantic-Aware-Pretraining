{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import functools\n",
    "import argparse\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "This code is heavily based on the TensorFlow preprocessing code from the T5 paper, available here:\n",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py\n",
    "\n",
    "Adapted for use with huggingface (torch) by Aaron Mueller.\n",
    "\"\"\"\n",
    "\n",
    "def to_dict(text, tokenizer, include_eos=True):\n",
    "    target = tokenizer.encode(text) if include_eos else tokenizer.encode(text)[:-1]\n",
    "    return {'inputs': \"\",\n",
    "            'targets': torch.tensor(target)}\n",
    "\n",
    "\n",
    "def load_data(in_file, tokenizer):\n",
    "    \"\"\"Expects input of the format\n",
    "    `{\"translation\": {\"src\": utterance, \"tgt\": label}}`.\n",
    "    Returns list of dictionaries of the following format:\n",
    "    {\"inputs\": \"\", \"targets\": tensor([encoded_text])}.\"\"\"\n",
    "    utterances, intents = [], []\n",
    "    punc = (\".\", \";\", \"!\", \"?\", \",\")\n",
    "    with open(in_file, 'r') as datastrings:\n",
    "        for datastring in datastrings:\n",
    "            data = json.loads(datastring)\n",
    "            utterance = data[\"translation\"][\"src\"].strip()\n",
    "            intent = data[\"translation\"][\"tgt\"].strip()\n",
    "            if not utterance.endswith(punc):\n",
    "                utterance += \".\"\n",
    "            if not intent.endswith(punc):\n",
    "                intent += \".\"\n",
    "\n",
    "            utterances.append(to_dict(utterance, tokenizer, include_eos=False))\n",
    "            intents.append(to_dict(intent, tokenizer, include_eos=True))\n",
    "    return (utterances, intents)\n",
    "\n",
    "\n",
    "def write_data(dataset, out_name, tokenizer):\n",
    "    with open(out_name, \"w\") as out_file:\n",
    "        for data in dataset:\n",
    "            data = {\"inputs\": tokenizer.decode(data[\"inputs\"]),\n",
    "                    \"targets\": tokenizer.decode(data[\"targets\"])}\n",
    "            json_obj = json.dumps(data)\n",
    "            out_file.write(json_obj + \"\\n\")\n",
    "\n",
    "\n",
    "def span_corruption(utterances, intents,\n",
    "                    sequence_length,\n",
    "                    mean_noise_span_length=3.0,\n",
    "                    0.15=0.15,\n",
    "                    seq_pack=False,\n",
    "                    label_semantics=\"multiple choice\",\n",
    "                    label_0.15=0.5):\n",
    "    \"\"\"Preprocessing for T5 denoising objective. Returns preprocessed\n",
    "    tokenized and encoded data.\n",
    "    Args:\n",
    "        dataset -- list of tensors (N, ?) where N is number of examples.\n",
    "                   tensor length depends on length of tokenized example.\n",
    "        sequence_length -- Maximum sequence length (default: 512)\n",
    "        seq_pack -- pack inputs into sequences of length approximately `sequence_length`.\n",
    "        label_semantics -- Whether and how to mask the utterance and intent. Can take the following values:\n",
    "                               None: only use utterances. Intents will not appear in the data.\n",
    "                               'concat': append intents to utterances, noise as if it were one full sequence.\n",
    "                               'full label': simply mask the entire label and none of the utterance.\n",
    "                               'separate': mask tokens in utterance with `0.15` probability, and mask\n",
    "                                           tokens in label with `label_0.15` probability.\n",
    "                               'label permute': try all possible ways of masking the tokens in the intent. Treat\n",
    "                                                each permutation as a new training example.\n",
    "                               'multiple choice': treat as a multiple choice problem. Give correct intent and\n",
    "                                                  a set of [2, 29] random intents with the utterance in the source\n",
    "                                                  sequence. Transduce to intent.\n",
    "    \"\"\"\n",
    "    if label_semantics is not None and label_semantics not in (\"full label\", \"label permute\", \"separate\",\n",
    "                                                               \"multiple choice\"):\n",
    "            raise ValueError(\"Unrecognized label masking strategy. Must be one of \"\n",
    "                             \"{'full label', 'label permute', 'separate'}.\")\n",
    "\n",
    "    input_length, targets_length = random_spans_helper(inputs_length=512)\n",
    "\n",
    "    if sequence_length['targets'] < targets_length:\n",
    "        # raise Exception(\"Exception not working?\")\n",
    "        raise ValueError(f'Expected targets length for span corruption ({targets_length}) is '\n",
    "                         f'greater than configured targets length '\n",
    "                         f\"({sequence_length['targets']})\")\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    proc_utterance_label_together = False\n",
    "    if label_semantics is None:\n",
    "        proc_utterance_label_together = True\n",
    "        ds = utterances\n",
    "    elif label_semantics == \"concat\":\n",
    "        proc_utterance_label_together = True\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            ds.append({'inputs': \"\",\n",
    "                       'targets': torch.cat((utterance[\"targets\"], intent[\"targets\"]))})\n",
    "    if proc_utterance_label_together:\n",
    "        ds = select_random_chunk(ds)    # deal with inputs longer than 512 tokens\n",
    "        if seq_pack:                    # pack sequences into training examples of ~512 tokens\n",
    "            ds = random_concat(ds)\n",
    "        ds = denoise(\n",
    "            ds,\n",
    "            tokenizer=tokenizer,\n",
    "            inputs_fn=noise_span_to_unique_sentinel,\n",
    "            targets_fn=nonnoise_span_to_unique_sentinel,\n",
    "            0.15=0.15,\n",
    "            noise_mask_fn=functools.partial(\n",
    "                random_spans_noise_mask,\n",
    "                mean_noise_span_length=mean_noise_span_length\n",
    "            )\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    if label_semantics == \"full label\":  # mask full label, not utterance\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            sentinel_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "            input = torch.cat((utterance[\"targets\"], torch.tensor([sentinel_id])))\n",
    "            target = torch.cat((torch.tensor([sentinel_id]), intent[\"targets\"]))\n",
    "            if input.shape[0] > 512:    # if seq too long, truncate\n",
    "                input = input[:512]\n",
    "            data = {'inputs': input,\n",
    "                    'targets': target}\n",
    "            ds.append(data)\n",
    "        return ds\n",
    "    if label_semantics == \"multiple choice\":\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            num_choices = int(random.uniform(2, 19))    # between 3 and 30 with correct intent\n",
    "            intents_list = random.sample(intents, num_choices)\n",
    "            intents_list.append(intent)\n",
    "            random.shuffle(intents_list)\n",
    "            # concatenate intent list (without eos tokens)\n",
    "            intents_choices = torch.cat([intent_item[\"targets\"][:-1] for intent_item in intents_list])\n",
    "            int_prefix = torch.tensor(tokenizer.encode(\"intents: \")[:-1])   # [:-1] gets rid of eos token\n",
    "            utt_prefix = torch.tensor(tokenizer.encode(\"utterance: \")[:-1])\n",
    "            eos_id = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "            source_tok = torch.cat((int_prefix, intents_choices, utt_prefix, utterance[\"targets\"],\n",
    "                                    torch.tensor([eos_id])))\n",
    "            if source_tok.shape[0] > 512:   # if seq too long, only give the correct intent.\n",
    "                source_tok = torch.cat((int_prefix, intent[\"targets\"][:-1], utt_prefix, utterance[\"targets\"],\n",
    "                                        torch.tensor([eos_id])))\n",
    "            if source_tok.shape[0] > 512:   # if seq still too long, truncate\n",
    "                source_tok = source_tok[:512]\n",
    "            data = {'inputs': source_tok, 'targets': intent[\"targets\"]}\n",
    "            ds.append(data)\n",
    "        return ds\n",
    "\n",
    "\n",
    "\"\"\"========== DRIVER CODE ==========\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    labelsemantics = \"concat\"\n",
    "\n",
    "    # SET RANDOM SEED FOR REPLICABLE BEHAVIOR\n",
    "    torch.manual_seed(1248)\n",
    "    random.seed(1248)\n",
    "\n",
    "    dataset = \"dataset/json/polyai-bank/polyai-bank_train.json\"\n",
    "\n",
    "    sequence_length = {'inputs': 512, 'targets': 512}\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    utterances, intents = load_data(dataset, tokenizer)\n",
    "    dataset = span_corruption(utterances, intents, sequence_length, seq_pack=False, label_semantics=labelsemantics)\n",
    "    write_data(dataset, \"random_noise.json\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "@dataclass\n",
    "class RandomNoise:\n",
    "    \"\"\"Randomly corrupts a span of tokens in the input.\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    def span_corruption(self, utterances, mean_noise_span_length=3.0, noise_density=0.15):\n",
    "\n",
    "        dataset = [self.get_random_segment(data, max_length=512) \n",
    "                   for data in list(filter(lambda x: x[\"targets\"].shape[0] > 0, utterances))]\n",
    "        \n",
    "        return self.denoise(dataset, noise_density=noise_density, noise_mask_fn=functools.partial(\n",
    "                self.random_spans_noise_mask,\n",
    "                mean_noise_span_length=mean_noise_span_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_random_segment(self, data, max_length):\n",
    "        \"\"\"Extract a chunk from the data, given a maximum length.\"\"\"\n",
    "        tokens = data[ \"targets\" ]\n",
    "        if tokens.shape[0] < max_length:\n",
    "            return {\"targets\": tokens}\n",
    "        start = torch.randint(0, tokens.shape[0] - max_length + 1, (1,)).item()\n",
    "        return {\"targets\": tokens[start: start + max_length]}\n",
    "\n",
    "\n",
    "    def random_spans_noise_mask(self, length, noise_density=0.15, mean_noise_span_length=3.0):\n",
    "        \"\"\"Calculate which spans to mask given input length.\n",
    "        Returns a vector of Booleans of length `length`, where `True`\n",
    "        corresponds to masking and `False` corresponds to keeping a token.\n",
    "        \"\"\"\n",
    "        orig_length = length\n",
    "        length = torch.tensor(length, dtype=torch.int32)\n",
    "        # avoid degenerate length values\n",
    "        length = torch.maximum(length, torch.tensor(2, dtype=torch.int32))\n",
    "        # helper functions for concise type conversion\n",
    "        def to_int(x):\n",
    "            return x.type(torch.int32)\n",
    "        def to_float(x):\n",
    "            return x.type(torch.float32)\n",
    "        # calculate number of noised and non-noised tokens\n",
    "        num_noise_tokens = to_int(torch.round(to_float(length) * noise_density))\n",
    "        num_noise_tokens = torch.minimum(\n",
    "            torch.maximum(num_noise_tokens, torch.tensor(1, dtype=torch.int32)), length-1)\n",
    "        num_noise_spans = to_int(\n",
    "            torch.round(to_float(num_noise_tokens) / mean_noise_span_length))\n",
    "        num_noise_spans = torch.maximum(num_noise_spans, torch.tensor(1, dtype=torch.int32))\n",
    "        num_nonnoise_tokens = length - num_noise_tokens\n",
    "        # pick lengths of noise spans and non-noise spans\n",
    "        def _random_segmentation(num_items, num_segments):\n",
    "            \"\"\"Partition items randomly into non-empty segments.\"\"\"\n",
    "            first_in_segment = torch.nn.functional.pad(\n",
    "                self.shuffle(to_int(torch.arange(num_items - 1) < num_segments - 1)),\n",
    "                [1, 0])\n",
    "            segment_id = torch.cumsum(first_in_segment, 0)\n",
    "            segment_length = self.segment_sum(torch.ones_like(segment_id), segment_id)\n",
    "            return segment_length\n",
    "\n",
    "        noise_span_lengths = _random_segmentation(\n",
    "            num_noise_tokens, num_noise_spans)\n",
    "        nonnoise_span_lengths = _random_segmentation(\n",
    "            num_nonnoise_tokens, num_noise_spans)\n",
    "        interleaved_span_lengths = torch.reshape(\n",
    "            torch.stack([nonnoise_span_lengths, noise_span_lengths], axis=1),\n",
    "                        [num_noise_spans * 2])\n",
    "        span_starts = torch.cumsum(interleaved_span_lengths, 0)[:-1]\n",
    "        span_start_indicator = self.segment_sum(\n",
    "            torch.ones_like(span_starts), span_starts, length)\n",
    "        span_num = torch.cumsum(span_start_indicator, 0)\n",
    "        is_noise = torch.eq(span_num % 2, torch.tensor(1, dtype=torch.int64))\n",
    "        return is_noise[:orig_length]\n",
    "\n",
    "\n",
    "    def denoise(self, dataset, noise_density=0.15, noise_mask_fn=None):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        def map_fn(features):\n",
    "            tokens = features['targets']\n",
    "            noise_mask = noise_mask_fn(tokens.shape[0], noise_density)\n",
    "            inputs = self.noise_span_to_unique_sentinel(tokens, noise_mask, vocab_size)\n",
    "            return {\n",
    "                'inputs': inputs,\n",
    "                'targets': self.nonnoise_span_to_unique_sentinel(tokens, noise_mask, vocab_size)\n",
    "            }\n",
    "        return [map_fn(data) for data in dataset]\n",
    "\n",
    "\n",
    "    def noise_span_to_unique_sentinel(self, tokens, noise_mask, vocab_size):\n",
    "        prev_token_is_noise = torch.nn.functional.pad(\n",
    "            noise_mask[:-1], [1, 0])\n",
    "\n",
    "        first_noise_tokens = torch.logical_and(\n",
    "            noise_mask, torch.logical_not(prev_token_is_noise))\n",
    "        subsequent_noise_tokens = torch.logical_and(\n",
    "            noise_mask, prev_token_is_noise)\n",
    "\n",
    "        sentinel = vocab_size - torch.cumsum(first_noise_tokens.int(), 0)\n",
    "\n",
    "        tokens = torch.where(first_noise_tokens, sentinel, tokens)\n",
    "        return torch.masked_select(tokens, torch.logical_not(subsequent_noise_tokens))\n",
    "\n",
    "\n",
    "    def nonnoise_span_to_unique_sentinel(self, tokens, noise_mask, vocab_size):\n",
    "        return self.noise_span_to_unique_sentinel(\n",
    "            tokens, torch.logical_not(noise_mask), vocab_size)\n",
    "\n",
    "\n",
    "    \"\"\"============= UTILITY FUNCTIONS ===============\"\"\"\n",
    "    def shuffle(self, value):\n",
    "        \"\"\"Randomly shuffle a tensor.\"\"\"\n",
    "        return value[torch.randperm(value.numel())].reshape(value.shape)\n",
    "\n",
    "    def segment_sum(self, data, segment_ids, num_segments=None):\n",
    "        \"\"\"Compute the sum along segments of a tensor.\"\"\"\n",
    "        if num_segments is None:\n",
    "            num_segments = segment_ids.unique().numel()\n",
    "        shape = [num_segments] + list(data.shape[1:])\n",
    "        return torch.zeros(*shape, dtype=data.dtype).scatter_add_(0, segment_ids, data)\n",
    "\n",
    "\n",
    "random_denoiser = RandomNoise( tokenizer=tokenizer )\n",
    "ds = random_denoiser.span_corruption(\n",
    "    utterances=utterances\n",
    ")\n",
    "write_data(ds, \"random_noise.json\", tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
