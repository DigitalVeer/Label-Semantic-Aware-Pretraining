{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples in None: 10003\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import functools\n",
    "import argparse\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "This code is heavily based on the TensorFlow preprocessing code from the T5 paper, available here:\n",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py\n",
    "\n",
    "Adapted for use with huggingface (torch) by Aaron Mueller.\n",
    "\"\"\"\n",
    "\n",
    "def to_dict(text, tokenizer, include_eos=True):\n",
    "    target = tokenizer.encode(text) if include_eos else tokenizer.encode(text)[:-1]\n",
    "    return {'inputs': \"\",\n",
    "            'targets': torch.tensor(target)}\n",
    "\n",
    "\n",
    "def load_data(in_file, tokenizer):\n",
    "    \"\"\"Expects input of the format\n",
    "    `{\"translation\": {\"src\": utterance, \"tgt\": label}}`.\n",
    "    Returns list of dictionaries of the following format:\n",
    "    {\"inputs\": \"\", \"targets\": tensor([encoded_text])}.\"\"\"\n",
    "    utterances, intents = [], []\n",
    "    punc = (\".\", \";\", \"!\", \"?\", \",\")\n",
    "    with open(in_file, 'r') as datastrings:\n",
    "        for datastring in datastrings:\n",
    "            data = json.loads(datastring)\n",
    "            utterance = data[\"translation\"][\"src\"].strip()\n",
    "            intent = data[\"translation\"][\"tgt\"].strip()\n",
    "            if not utterance.endswith(punc):\n",
    "                utterance += \".\"\n",
    "            if not intent.endswith(punc):\n",
    "                intent += \".\"\n",
    "\n",
    "            utterances.append(to_dict(utterance, tokenizer, include_eos=False))\n",
    "            intents.append(to_dict(intent, tokenizer, include_eos=True))\n",
    "    return (utterances, intents)\n",
    "\n",
    "\n",
    "def write_data(dataset, out_name, tokenizer):\n",
    "    with open(out_name, \"w\") as out_file:\n",
    "        for data in dataset:\n",
    "            data = {\"inputs\": tokenizer.decode(data[\"inputs\"]),\n",
    "                    \"targets\": tokenizer.decode(data[\"targets\"])}\n",
    "            json_obj = json.dumps(data)\n",
    "            out_file.write(json_obj + \"\\n\")\n",
    "\n",
    "\n",
    "def span_corruption(utterances, intents,\n",
    "                    sequence_length,\n",
    "                    mean_noise_span_length=3.0,\n",
    "                    noise_density=0.15,\n",
    "                    seq_pack=False,\n",
    "                    label_semantics=\"multiple choice\",\n",
    "                    label_noise_density=0.5):\n",
    "    \"\"\"Preprocessing for T5 denoising objective. Returns preprocessed\n",
    "    tokenized and encoded data.\n",
    "    Args:\n",
    "        dataset -- list of tensors (N, ?) where N is number of examples.\n",
    "                   tensor length depends on length of tokenized example.\n",
    "        sequence_length -- Maximum sequence length (default: 512)\n",
    "        seq_pack -- pack inputs into sequences of length approximately `sequence_length`.\n",
    "        label_semantics -- Whether and how to mask the utterance and intent. Can take the following values:\n",
    "                               None: only use utterances. Intents will not appear in the data.\n",
    "                               'concat': append intents to utterances, noise as if it were one full sequence.\n",
    "                               'full label': simply mask the entire label and none of the utterance.\n",
    "                               'separate': mask tokens in utterance with `noise_density` probability, and mask\n",
    "                                           tokens in label with `label_noise_density` probability.\n",
    "                               'label permute': try all possible ways of masking the tokens in the intent. Treat\n",
    "                                                each permutation as a new training example.\n",
    "                               'multiple choice': treat as a multiple choice problem. Give correct intent and\n",
    "                                                  a set of [2, 29] random intents with the utterance in the source\n",
    "                                                  sequence. Transduce to intent.\n",
    "    \"\"\"\n",
    "    if label_semantics is not None and label_semantics not in (\"full label\", \"label permute\", \"separate\",\n",
    "                                                               \"multiple choice\", \"concat\"):\n",
    "            raise ValueError(\"Unrecognized label masking strategy. Must be one of \"\n",
    "                             \"{'full label', 'label permute', 'separate'}.\")\n",
    "\n",
    "    input_length, targets_length = random_spans_helper(inputs_length=512)\n",
    "\n",
    "    if sequence_length['targets'] < targets_length:\n",
    "        # raise Exception(\"Exception not working?\")\n",
    "        raise ValueError(f'Expected targets length for span corruption ({targets_length}) is '\n",
    "                         f'greater than configured targets length '\n",
    "                         f\"({sequence_length['targets']})\")\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    proc_utterance_label_together = False\n",
    "    if label_semantics is None:\n",
    "        proc_utterance_label_together = True\n",
    "        ds = utterances\n",
    "    elif label_semantics == \"concat\":\n",
    "        proc_utterance_label_together = True\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            ds.append({'inputs': \"\",\n",
    "                       'targets': torch.cat((utterance[\"targets\"], intent[\"targets\"]))})\n",
    "    if proc_utterance_label_together:\n",
    "        ds = select_random_chunk(ds)    # deal with inputs longer than 512 tokens\n",
    "        if seq_pack:                    # pack sequences into training examples of ~512 tokens\n",
    "            ds = random_concat(ds)\n",
    "        ds = denoise(\n",
    "            ds,\n",
    "            tokenizer=tokenizer,\n",
    "            inputs_fn=noise_span_to_unique_sentinel,\n",
    "            targets_fn=nonnoise_span_to_unique_sentinel,\n",
    "            noise_density=noise_density,\n",
    "            noise_mask_fn=functools.partial(\n",
    "                random_spans_noise_mask,\n",
    "                mean_noise_span_length=mean_noise_span_length\n",
    "            )\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    if label_semantics == \"full label\":  # mask full label, not utterance\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            sentinel_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "            input = torch.cat((utterance[\"targets\"], torch.tensor([sentinel_id])))\n",
    "            target = torch.cat((torch.tensor([sentinel_id]), intent[\"targets\"]))\n",
    "            if input.shape[0] > 512:    # if seq too long, truncate\n",
    "                input = input[:512]\n",
    "            data = {'inputs': input,\n",
    "                    'targets': target}\n",
    "            ds.append(data)\n",
    "        return ds\n",
    "    if label_semantics == \"multiple choice\":\n",
    "        ds = []\n",
    "        for utterance, intent in zip(utterances, intents):\n",
    "            num_choices = int(random.uniform(2, 19))    # between 3 and 30 with correct intent\n",
    "            intents_list = random.sample(intents, num_choices)\n",
    "            intents_list.append(intent)\n",
    "            random.shuffle(intents_list)\n",
    "            # concatenate intent list (without eos tokens)\n",
    "            intents_choices = torch.cat([intent_item[\"targets\"][:-1] for intent_item in intents_list])\n",
    "            int_prefix = torch.tensor(tokenizer.encode(\"intents: \")[:-1])   # [:-1] gets rid of eos token\n",
    "            utt_prefix = torch.tensor(tokenizer.encode(\"utterance: \")[:-1])\n",
    "            eos_id = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "            source_tok = torch.cat((int_prefix, intents_choices, utt_prefix, utterance[\"targets\"],\n",
    "                                    torch.tensor([eos_id])))\n",
    "            if source_tok.shape[0] > 512:   # if seq too long, only give the correct intent.\n",
    "                source_tok = torch.cat((int_prefix, intent[\"targets\"][:-1], utt_prefix, utterance[\"targets\"],\n",
    "                                        torch.tensor([eos_id])))\n",
    "            if source_tok.shape[0] > 512:   # if seq still too long, truncate\n",
    "                source_tok = source_tok[:512]\n",
    "            data = {'inputs': source_tok, 'targets': intent[\"targets\"]}\n",
    "            ds.append(data)\n",
    "        return ds\n",
    "\n",
    "\n",
    "    # TODO: implement other label masking strategies\n",
    "\n",
    "\n",
    "\n",
    "def random_spans_helper(inputs_length=512, noise_density=0.15,\n",
    "                        mean_noise_span_length=3.0,\n",
    "                        extra_tokens_per_span_inputs=1,\n",
    "                        extra_tokens_per_span_targets=1):\n",
    "    \"\"\"Helps us avoid padding when masking inputs.\n",
    "    Assumes that EOS token will be appended to examples.\"\"\"\n",
    "\n",
    "    def _tokens_length_to_inputs_length_targets_length(tokens_length):\n",
    "        num_noise_tokens = int(round(tokens_length * noise_density))\n",
    "        num_nonnoise_tokens = tokens_length - num_noise_tokens\n",
    "        num_noise_spans = int(round(num_noise_tokens / mean_noise_span_length))\n",
    "        return (\n",
    "            num_nonnoise_tokens +\n",
    "            num_noise_spans * extra_tokens_per_span_inputs + 1,\n",
    "            num_noise_tokens +\n",
    "            num_noise_spans * extra_tokens_per_span_targets + 1)\n",
    "\n",
    "    tokens_length = inputs_length\n",
    "    while (_tokens_length_to_inputs_length_targets_length(tokens_length + 1)[0] <= inputs_length):\n",
    "        tokens_length += 1\n",
    "    inputs_length, targets_length = (_tokens_length_to_inputs_length_targets_length(tokens_length))\n",
    "    return tokens_length, targets_length\n",
    "\n",
    "\n",
    "def select_random_chunk(dataset,\n",
    "                        feature_key='targets',\n",
    "                        max_length=512):\n",
    "    \"\"\"Extract one span of at most `max_length` tokens.\n",
    "    If token sequence longer than `max_length`, return a random subsequence.\n",
    "    If token sequence shorter than `max_length`, return the original sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter empty examples\n",
    "    dataset = [example for example in dataset if example[feature_key].shape[0] > 0]\n",
    "    # Select random chunk of tokens\n",
    "    def _my_fn(data):\n",
    "        tokens = data[feature_key]\n",
    "        if tokens.shape[0] < max_length:\n",
    "            return {feature_key: tokens}\n",
    "        n_tokens = torch.tensor(tokens.shape[0])\n",
    "        num_segments = torch.ceil(n_tokens.float() /\n",
    "                                  torch.tensor(max_length, dtype=torch.float32)).type(torch.int32)\n",
    "        start = (max_length * (-num_segments * torch.rand([]) + num_segments)).int()\n",
    "        end = torch.minimum(start + max_length, n_tokens)\n",
    "        chunk = {feature_key: tokens[start:end]}\n",
    "        return chunk\n",
    "    return [_my_fn(data) for data in dataset]\n",
    "\n",
    "\n",
    "def random_concat(dataset, max_length=512, feature_key='targets'):\n",
    "    \"\"\"Pack random sequences together into training examples (w/o replacement).\n",
    "    NOTE: expects all sequences to have length <= `max_length`! Be sure to run\n",
    "    `select_random_chunk` on the data before running this function.\"\"\"\n",
    "    random.shuffle(dataset)\n",
    "    new_dataset = []\n",
    "    len_example = 0\n",
    "    example = torch.tensor((), dtype=torch.int32)\n",
    "    for data in dataset:\n",
    "        len_example += data[feature_key].shape[0]\n",
    "        if len_example >= max_length - 2:\n",
    "            new_dataset.append({feature_key: example})\n",
    "            example = data[feature_key]\n",
    "            len_example = example.shape[0]\n",
    "            continue\n",
    "        example = torch.cat((example, data[feature_key]))\n",
    "    # add final example to dataset\n",
    "    new_dataset.append({feature_key: example})\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def random_spans_noise_mask(length,\n",
    "                            noise_density=0.15,\n",
    "                            mean_noise_span_length=3.0):\n",
    "    \"\"\"Calculate which spans to mask given input length.\n",
    "    Returns a vector of Booleans of length `length`, where `True`\n",
    "    corresponds to masking and `False` corresponds to keeping a token.\n",
    "    \"\"\"\n",
    "    orig_length = length\n",
    "    length = torch.tensor(length, dtype=torch.int32)\n",
    "    # avoid degenerate length values\n",
    "    length = torch.maximum(length, torch.tensor(2, dtype=torch.int32))\n",
    "    # helper functions for concise type conversion\n",
    "    def to_int(x):\n",
    "        return x.type(torch.int32)\n",
    "    def to_float(x):\n",
    "        return x.type(torch.float32)\n",
    "    # calculate number of noised and non-noised tokens\n",
    "    num_noise_tokens = to_int(torch.round(to_float(length) * noise_density))\n",
    "    num_noise_tokens = torch.minimum(\n",
    "        torch.maximum(num_noise_tokens, torch.tensor(1, dtype=torch.int32)), length-1)\n",
    "    num_noise_spans = to_int(\n",
    "        torch.round(to_float(num_noise_tokens) / mean_noise_span_length))\n",
    "    num_noise_spans = torch.maximum(num_noise_spans, torch.tensor(1, dtype=torch.int32))\n",
    "    num_nonnoise_tokens = length - num_noise_tokens\n",
    "    # pick lengths of noise spans and non-noise spans\n",
    "    def _random_segmentation(num_items, num_segments):\n",
    "        \"\"\"Partition items randomly into non-empty segments.\"\"\"\n",
    "        first_in_segment = torch.nn.functional.pad(\n",
    "            shuffle(to_int(torch.arange(num_items - 1) < num_segments - 1)),\n",
    "            [1, 0])\n",
    "        segment_id = torch.cumsum(first_in_segment, 0)\n",
    "        segment_length = segment_sum(torch.ones_like(segment_id), segment_id)\n",
    "        return segment_length\n",
    "\n",
    "    noise_span_lengths = _random_segmentation(\n",
    "        num_noise_tokens, num_noise_spans)\n",
    "    nonnoise_span_lengths = _random_segmentation(\n",
    "        num_nonnoise_tokens, num_noise_spans)\n",
    "    interleaved_span_lengths = torch.reshape(\n",
    "        torch.stack([nonnoise_span_lengths, noise_span_lengths], axis=1),\n",
    "                    [num_noise_spans * 2])\n",
    "    span_starts = torch.cumsum(interleaved_span_lengths, 0)[:-1]\n",
    "    span_start_indicator = segment_sum(\n",
    "        torch.ones_like(span_starts), span_starts, length)\n",
    "    span_num = torch.cumsum(span_start_indicator, 0)\n",
    "    is_noise = torch.eq(span_num % 2, torch.tensor(1, dtype=torch.int64))\n",
    "    return is_noise[:orig_length]\n",
    "\n",
    "\n",
    "def denoise(dataset,\n",
    "            noise_density=0.15,\n",
    "            noise_mask_fn=None,\n",
    "            inputs_fn=None,\n",
    "            targets_fn=None,\n",
    "            tokenizer=None):\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    def map_fn(features):\n",
    "        tokens = features['targets']\n",
    "        noise_mask = noise_mask_fn(tokens.shape[0], noise_density)\n",
    "        inputs = inputs_fn(tokens, noise_mask, vocab_size)\n",
    "        if targets_fn:\n",
    "            targets = targets_fn(tokens, noise_mask, vocab_size)\n",
    "        else:\n",
    "            targets = tokens\n",
    "        return {'inputs': inputs, 'targets': targets}\n",
    "    return [map_fn(data) for data in dataset]\n",
    "\n",
    "\n",
    "def noise_span_to_unique_sentinel(tokens, noise_mask, vocab_size):\n",
    "    prev_token_is_noise = torch.nn.functional.pad(\n",
    "        noise_mask[:-1], [1, 0])\n",
    "\n",
    "    first_noise_tokens = torch.logical_and(\n",
    "        noise_mask, torch.logical_not(prev_token_is_noise))\n",
    "    subsequent_noise_tokens = torch.logical_and(\n",
    "        noise_mask, prev_token_is_noise)\n",
    "\n",
    "    sentinel = vocab_size - torch.cumsum(first_noise_tokens.int(), 0)\n",
    "\n",
    "    tokens = torch.where(first_noise_tokens, sentinel, tokens)\n",
    "    return torch.masked_select(tokens, torch.logical_not(subsequent_noise_tokens))\n",
    "\n",
    "\n",
    "def nonnoise_span_to_unique_sentinel(tokens, noise_mask, vocab_size):\n",
    "    return noise_span_to_unique_sentinel(\n",
    "        tokens, torch.logical_not(noise_mask), vocab_size)\n",
    "\n",
    "\n",
    "\"\"\"============= UTILITY FUNCTIONS ===============\"\"\"\n",
    "def shuffle(value):\n",
    "    \"\"\"Randomly shuffle a tensor.\"\"\"\n",
    "    flat_value = torch.reshape(value, [-1])\n",
    "    indices = torch.argsort(\n",
    "        torch.rand(flat_value.shape)\n",
    "    )\n",
    "    flat_shuffle = torch.gather(flat_value, 0, indices)\n",
    "    return torch.reshape(flat_shuffle, value.shape)\n",
    "\n",
    "\n",
    "def segment_sum(data, segment_ids, num_segments=None):\n",
    "    \"\"\"Compute the sum along segments of a tensor.\"\"\"\n",
    "    if num_segments is None:\n",
    "        num_segments = len(torch.unique(segment_ids))\n",
    "    if len(segment_ids.shape) == 1:\n",
    "        s = torch.prod(torch.tensor(data.shape[1:])).long()\n",
    "        segment_ids = segment_ids.repeat_interleave(s).view(segment_ids.shape[0],\n",
    "                                                            *data.shape[1:])\n",
    "\n",
    "    shape = [num_segments] + list(data.shape[1:])\n",
    "    tensor = torch.zeros(*shape).scatter_add(0, segment_ids, data.float())\n",
    "    tensor = tensor.type(data.dtype)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\"\"\"========== DRIVER CODE ==========\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    \n",
    "    # The authors provide the following formats:\n",
    "    # [\"full label\", \"label permute\", \"separate\",\"multiple choice\", \"concat\"]\n",
    "    #However, label permute, and separate, are not implemented by the authors.\n",
    "\n",
    "    formats = [\"full label\", \"multiple choice\", \"concat\", None]\n",
    "\n",
    "    # SET RANDOM SEED FOR REPLICABLE BEHAVIOR\n",
    "    torch.manual_seed(1248)\n",
    "    random.seed(1248)\n",
    "\n",
    "    dataset = \"dataset/json/polyai-bank/polyai-bank_train.json\"\n",
    "\n",
    "    sequence_length = {'inputs': 512, 'targets': 512}\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    utterances, intents = load_data(dataset, tokenizer)\n",
    "    for labelsemantics in formats:\n",
    "        dataset = span_corruption(utterances, intents, sequence_length, seq_pack=False, label_semantics=labelsemantics)\n",
    "        write_data(dataset, f\"{labelsemantics}_polybank.json\", tokenizer)\n",
    "        print(f\"Num examples in {labelsemantics}: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
