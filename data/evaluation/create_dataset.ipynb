{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSAP Data Preprocessing\n",
    "---\n",
    "This notebook contains code for combining the pretraining datasets and creating a training, testing, and validation splits for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "def green(text):\n",
    "    return f\"\\033[92m\\033[1m{text}\\033[0m\"\n",
    "\n",
    "def highlight(text):\n",
    "    return f'\\x1b[6;30;42m{text}\\x1b[0m'\n",
    "\n",
    "\n",
    "#Path settings\n",
    "CURR_PATH = \"\"\n",
    "DATA_PATH = os.path.join( CURR_PATH, 'dataset' )\n",
    "\n",
    "#Where to save the data\n",
    "csv_cache  = f\"{DATA_PATH}/csv\"\n",
    "json_cache = f\"{DATA_PATH}/json\"\n",
    "\n",
    "def create_dir( path ):\n",
    "    os.makedirs( path, exist_ok=True )\n",
    "\n",
    "#Create directories\n",
    "create_dir( csv_cache )\n",
    "create_dir( json_cache )\n",
    "\n",
    "#Path settings\n",
    "JSON_PATH  = json_cache\n",
    "COMBINED_JSON_PATH = f\"{ JSON_PATH }/combined\"\n",
    "\n",
    "# All data folders\n",
    "data_folders = ['ATIS', 'SNIPS', 'TOPS_Reminder', 'TOPS_Weather']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handler\n",
    "---\n",
    "Below, we use a class to easily handle writing/reading/preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DFHandler:\n",
    "    folder_name: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.data = pd.concat([self.data, data])\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Gets the data from the data folder.\"\"\"\n",
    "        curr_folder = os.path.join(CURR_PATH, self.folder_name)\n",
    "        files = os.listdir(f'{curr_folder}/data')\n",
    "        return {file: pd.read_csv(f'{curr_folder}/data/{file}') for file in files}\n",
    "        \n",
    "    \n",
    "    def write_to_json( self, df, output_file ):\n",
    "        with open( output_file, 'w' ) as out_data:\n",
    "            for _, row in df.iterrows():\n",
    "                utterance = row[\"text\"]\n",
    "                intent    = row[\"intent\"]\n",
    "\n",
    "                json_obj = json.dumps({\"translation\":\n",
    "                    {\"src\": utterance, \"tgt\": intent, \"prefix\": \"intent classification: \"}\n",
    "                })\n",
    "                out_data.write(json_obj + '\\n')\n",
    "\n",
    "    def create_datasets( self ):\n",
    "        all_data = self.get_data()\n",
    "        for file, df in all_data.items():\n",
    "                       \n",
    "            #Create naming scheme and remove \".csv\"\n",
    "            folder_file = f\"{self.folder_name}_{file}\".replace(\".csv\", \"\")\n",
    "\n",
    "            #Check if folder exists\n",
    "            if not os.path.exists( f\"{csv_cache}/{self.folder_name}\" ):\n",
    "                os.makedirs( f\"{csv_cache}/{self.folder_name}\" )\n",
    "\n",
    "            if not os.path.exists( f\"{json_cache}/{self.folder_name}\" ):\n",
    "                os.makedirs( f\"{json_cache}/{self.folder_name}\" )\n",
    "\n",
    "            #Save to csv and json\n",
    "            df.to_csv( f'{csv_cache}/{self.folder_name}/{folder_file}.csv' )\n",
    "            self.write_to_json( df, f'{json_cache}/{self.folder_name}/{folder_file}.json' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to JSON\n",
    "---\n",
    "\n",
    "Below, we write the same datasets to their respective JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1mCreating dataset for ATIS\u001b[0m\n",
      "Dataset created for ATIS.\n",
      "CSV Location: dataset/csv/ATIS\n",
      "JSON Location: dataset/json/ATIS\n",
      "\n",
      "\u001b[92m\u001b[1mCreating dataset for SNIPS\u001b[0m\n",
      "Dataset created for SNIPS.\n",
      "CSV Location: dataset/csv/SNIPS\n",
      "JSON Location: dataset/json/SNIPS\n",
      "\n",
      "\u001b[92m\u001b[1mCreating dataset for TOPS_Reminder\u001b[0m\n",
      "Dataset created for TOPS_Reminder.\n",
      "CSV Location: dataset/csv/TOPS_Reminder\n",
      "JSON Location: dataset/json/TOPS_Reminder\n",
      "\n",
      "\u001b[92m\u001b[1mCreating dataset for TOPS_Weather\u001b[0m\n",
      "Dataset created for TOPS_Weather.\n",
      "CSV Location: dataset/csv/TOPS_Weather\n",
      "JSON Location: dataset/json/TOPS_Weather\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for folder in data_folders:\n",
    "    print(green(f'Creating dataset for {folder}'))\n",
    "    dh = DFHandler( folder )\n",
    "    dh.create_datasets()\n",
    "    print(f\"Dataset created for {folder}.\\nCSV Location: {csv_cache}/{folder}\\nJSON Location: {json_cache}/{folder}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
