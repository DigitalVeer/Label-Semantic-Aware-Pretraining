{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84382e45-42f0-4449-854f-ec13ed2f2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, Seq2SeqTrainer, TrainingArguments, DataCollatorForSeq2Seq, AutoModelForSequenceClassification, Seq2SeqTrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sys import argv as args\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "317e5abc-89c8-4ca1-b08f-cbf362784d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/work/pi_adrozdov_umass_edu/vpamidimukka_umass_edu/hf_cache/datasets/csv/default-203601fc4f379794/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff194b9010e64fca87b0fd20b4562000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"intent classification: \"\n",
    "    \n",
    "model_name_or_path = '../output/fine-tune_models/SNIPS/model_2/four_shot'\n",
    "test_set_path = '../data/evaluation/SNIPS/data/test.csv'\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=test_set_path)\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3764a4d0-4cb2-4f0b-99c5-722c0c81f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "59e35e88-ebfa-4952-b6b3-fa2c47dd64a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_adrozdov_umass_edu/vpamidimukka_umass_edu/envs/vadops/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = [prefix + doc for doc in dataset[\"text\"]]\n",
    "model_inputs = tokenizer(inputs, max_length=1024, padding = True, return_tensors=\"pt\")\n",
    "labels = tokenizer(dataset[\"intent\"], max_length=1024, padding = True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0aabf9ba-17ba-4765-a9ab-584a73fef967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 9508, 13774,    10,  ...,     0,     0,     0],\n",
       "         [ 9508, 13774,    10,  ...,     0,     0,     0],\n",
       "         [ 9508, 13774,    10,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 9508, 13774,    10,  ...,     0,     0,     0],\n",
       "         [ 9508, 13774,    10,  ...,     0,     0,     0],\n",
       "         [ 9508, 13774,    10,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[2334,  304, 2911, 3350,    5,    1],\n",
       "         [2334,  304, 2911, 3350,    5,    1],\n",
       "         [2334,  304, 2911, 3350,    5,    1],\n",
       "         ...,\n",
       "         [4769, 9937,   53, 8042,    5,    1],\n",
       "         [4769, 9937,   53, 8042,    5,    1],\n",
       "         [4769, 9937,   53, 8042,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "96903320-30bb-49bc-89e3-ae092a1b7b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_adrozdov_umass_edu/vpamidimukka_umass_edu/envs/vadops/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "outputs = model.generate(model_inputs['input_ids'].to(device))\n",
    "# outputs = model.g(input_ids = , labels = labels['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2a2ad726-6cac-4e11-b3c2-7e62a6b4c240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "wrong_examples = []\n",
    "for i in range(len(outputs)):\n",
    "    out = tokenizer.decode(outputs[i], skip_special_tokens= True)\n",
    "    inp = tokenizer.decode(labels['input_ids'][i], skip_special_tokens= True)\n",
    "    if inp != out:\n",
    "        wrong_examples.append([dataset[\"text\"][i],inp, out])\n",
    "        counter += 1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ca5d9c8b-8449-4127-abb8-f13202aa8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(wrong_examples)\n",
    "df.columns = ['text', 'true_label', 'predicted_label']\n",
    "df.to_csv('../analysis/wrong_examples.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vpamidimukka_umass_edu-vadops)",
   "language": "python",
   "name": "conda-env-vpamidimukka_umass_edu-vadops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
