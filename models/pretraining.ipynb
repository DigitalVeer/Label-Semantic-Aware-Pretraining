{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a1774b-191c-487b-a5fc-eb1289d81dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc0e1a-e1a7-4414-9fe0-21113d9cab9a",
   "metadata": {},
   "source": [
    "### Model Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53f7a7-166d-43db-ba00-2a32f2a2b5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 't5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf943dd-b874-4bbb-b772-e443ffb0cdb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32085cb2-1b16-44cc-b795-1c1284500776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {\n",
    "    'train_file': '/path/to/training_data_file',\n",
    "    'validation_file': '/path/to/validation_data_file',\n",
    "    'max_target_length': 128,\n",
    "    'max_source_length': 512,\n",
    "    'ignore_pad_token_for_loss': True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb90c6-dd6b-46ec-ae11-259bbd81832c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce8790-5765-4342-a310-fa5f1966461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'output_dir': './output',\n",
    "    'predict_with_generate': False,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 5e-4,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'num_train_epochs': 10,\n",
    "    'save_total_limit': 4,\n",
    "    'save_strategy': 'epoch',\n",
    "    'seed': 42\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0f25b-3a2c-43b4-a6db-466ddeb83255",
   "metadata": {},
   "source": [
    "## Setting up the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe829f2-824e-4f70-bc09-eee8804c2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(training_args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6316f97-9d98-4ae2-ba62-9c861a937c97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58104cdc-840b-490d-93d0-c09a3cd4e465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = { 'train': data_args['train_file'], 'validation': data_args['validation_file'] }\n",
    "datasets = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73bf1d-e514-4ff9-bde1-d27d646aef21",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8bfed-2dca-4893-a2f2-0057fd12b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast = True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config = config,\n",
    "    from_tf=bool('.ckpt' in model_name_or_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b3663-032d-4050-a83c-594874a01dcc",
   "metadata": {},
   "source": [
    "## Tokenize the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1800c1-52fa-4c30-9599-954f6fff6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['inputs']]\n",
    "    targets = [ex for ex in examples['targets']]\n",
    "    model_inputs = tokenizer(inputs, max_length= data_args['max_source_length'], padding = False, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length = data_args['max_target_length'], padding = False, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bad4c8-8d2d-4598-b566-365dee7f4e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = datasets['train'], datasets['validation']\n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ec022-79d7-48c9-946d-4d2379305abc",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ad8ec-32f7-4845-a596-08f6d9a485bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "label_pad_token_id = -100 if data_args['ignore_pad_token_for_loss'] else tokenizer.pad_token_id\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, label_pad_token_id=label_pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc2d21-67db-4ca3-a1ff-2a13d3a08081",
   "metadata": {},
   "source": [
    "### Initialise Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af078d7-a039-4f7a-93a1-bb49d35d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfomers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065fd90-776e-4a87-af55-8cb4e1ac0866",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947cb35-9f6a-4ff0-bb22-7cf87c1872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_result = trainer.train(model_path=model_name_or_path if os.path.isdir(model_name_or_path) else None)\n",
    "trainer.save_model()\n",
    "\n",
    "output_train_file = os.path.join(training_args['output_dir'], 'train_results.txt')\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_train_file, 'w') as writer:\n",
    "        for key, value in sorted(train_result.metrics.items()):\n",
    "            writer.write(f'{key} = {value}\\n')\n",
    "\n",
    "    # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n",
    "    trainer.state.save_to_json(os.path.join(training_args['output_dir'], 'trainer_state.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e4184-03cc-4e21-b777-bcc0118cf253",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccde40-d025-4b44-913c-4007cc6d8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate_predictions(pred_filename, gold_filename):\n",
    "    with open(pred_filename, 'r') as pred_f, open(gold_filename) as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "    \n",
    "        total = 0.0\n",
    "        full_correct = 0.0\n",
    "        first_correct = 0.0\n",
    "        \n",
    "        for i in range(len(pred_lines)):\n",
    "            pred_line = pred_lines[i].strip()\n",
    "            if gold_filename.endswith('.json'):\n",
    "                gold_json = json.loads(gold_lines[i])\n",
    "                gold_line = gold_json['translation']['tgt']\n",
    "            else:  \n",
    "                gold_line = gold_lines[i].strip().split('\\t')[1]\n",
    "            \n",
    "            # remove space before period/question mark\n",
    "            gold_line = gold_line.replace(' ?', '?').replace(' .', '.').replace(' ,', ',') \n",
    "\n",
    "            total +=1\n",
    "\n",
    "            if pred_line == gold_line:\n",
    "                full_correct += 1\n",
    "                first_correct += 1\n",
    "            else:\n",
    "                pred_words = pred_line.split()\n",
    "                gold_words = gold_line.split()\n",
    "                if len(pred_words) > 0 and pred_words[0] == gold_words[0]:\n",
    "                    first_correct += 1\n",
    "\n",
    "  \n",
    "    return  (first_correct / total), (full_correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7ef12-1836-4b06-bbb8-ce6de1c654a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = os.path.basename(data_args['validation_file']).replace('.json', '')\n",
    "\n",
    "predictions = trainer.predict(test_dataset=eval_dataset, max_length=100)\n",
    "output_pred_file = os.path.join(training_args['output_dir'], basename + '.eval_preds_seq2seq.txt')\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_pred_file, 'w') as writer:\n",
    "        for pred in tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True):\n",
    "            writer.write(pred + '\\n')\n",
    "\n",
    "output_eval_file = os.path.join(training_args['output_dir'], basename + '.eval_results_seq2seq.txt')\n",
    "first_acc, full_acc = evaluate_predictions(output_pred_file, data_args['validation_file'])\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_eval_file, 'w') as writer:\n",
    "        writer.write(f'Exact match accuracy: {full_acc}\\n')\n",
    "        writer.write(f'First word accuracy: {first_acc}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vpamidimukka_umass_edu-vadops)",
   "language": "python",
   "name": "conda-env-vpamidimukka_umass_edu-vadops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
