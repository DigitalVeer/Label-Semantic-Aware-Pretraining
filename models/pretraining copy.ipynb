{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2a1774b-191c-487b-a5fc-eb1289d81dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Arguments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2dc0e1a-e1a7-4414-9fe0-21113d9cab9a",
   "metadata": {},
   "source": [
    "### Model Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca53f7a7-166d-43db-ba00-2a32f2a2b5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 't5-small'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf943dd-b874-4bbb-b772-e443ffb0cdb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32085cb2-1b16-44cc-b795-1c1284500776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {\n",
    "    'train_file': 'train.json',\n",
    "    'validation_file': 'test.json',\n",
    "    'max_target_length': 128,\n",
    "    'max_source_length': 512,\n",
    "    'ignore_pad_token_for_loss': True,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bb90c6-dd6b-46ec-ae11-259bbd81832c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce8790-5765-4342-a310-fa5f1966461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'model_name_or_path': model_name_or_path,\n",
    "    'output_dir': './output',\n",
    "    'predict_with_generate': False,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': 5e-4,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'num_train_epochs': 10,\n",
    "    'save_total_limit': 4,\n",
    "    'save_strategy': 'epoch',\n",
    "    'seed': 42\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2b0f25b-3a2c-43b4-a6db-466ddeb83255",
   "metadata": {},
   "source": [
    "## Setting up the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe829f2-824e-4f70-bc09-eee8804c2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(training_args['seed'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6316f97-9d98-4ae2-ba62-9c861a937c97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58104cdc-840b-490d-93d0-c09a3cd4e465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/digit/.cache/huggingface/datasets/json/default-474954548f6c3757/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4f94124a64823ba620ba62d044ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec7fee16cb4a18b3c3f77663f67742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d14645f18742769df525b3ed5baed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset('json', data_files={'train': 'train.json', 'test': 'test.json'})\n",
    "\n",
    "# Define a function for preprocessing\n",
    "def preprocess_function(examples):\n",
    "    processed_examples = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': []\n",
    "    }\n",
    "\n",
    "    for i, t in zip(examples['inputs'], examples['targets']):\n",
    "        tokenized_inputs = tokenizer(i, max_length=512, truncation=True, padding='max_length')\n",
    "        tokenized_targets = tokenizer(t, max_length=38, truncation=True, padding='max_length')\n",
    "        \n",
    "        if len(tokenized_inputs['input_ids']) <= 512:\n",
    "            processed_examples['input_ids'].append(tokenized_inputs['input_ids'])\n",
    "            processed_examples['attention_mask'].append(tokenized_inputs['attention_mask'])\n",
    "            processed_examples['labels'].append(tokenized_targets['input_ids'])\n",
    "\n",
    "    return processed_examples\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b334a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest input: [21972, 8, 9387, 3, 16042, 7, 3615, 12, 2, 96, 23728, 1265, 52, 40, 121, 10, 121, 5948, 7, 10, 2, 3, 87, 2, 3, 87, 1986, 5, 17193, 4067, 5, 287, 2, 3, 87, 8221, 7, 2, 3, 14785, 2, 3, 20376, 2, 3, 87, 196, 6399, 7, 210, 7059, 4554, 106, 5411, 102, 1725, 121, 976, 12911, 1265, 52, 40, 121, 10, 121, 5948, 7, 10, 2, 3, 87, 2, 3, 87, 1986, 5, 17193, 4067, 5, 287, 2, 3, 87, 8221, 7, 2, 3, 14785, 2, 3, 20376, 2, 3, 87, 196, 6399, 7, 210, 7059, 4554, 106, 5411, 102, 1725, 121, 976, 23728, 518, 23, 26, 189, 121, 10, 25991, 976, 23728, 3845, 2632, 121, 10, 5426, 976, 12911, 518, 23, 26, 189, 121, 10, 28212, 976, 12911, 3845, 2632, 121, 10, 3647, 25134, 5373, 2517, 3288, 2368, 6348, 2469, 976, 2176, 35, 7, 53, 121, 10, 121, 2, 3, 8481, 853, 2423, 2, 96, 51, 210, 18, 1893, 7, 49, 18, 670, 2562, 2, 96, 3155, 2, 3, 102, 3155, 196, 8307, 48, 18721, 13, 46, 3146, 2, 3, 29, 2, 3, 87, 102, 3155, 2, 3, 102, 3155, 434, 447, 5167, 10, 2, 3, 9, 8318, 2423, 2, 96, 29, 32, 25278, 2, 96, 853, 2423, 2, 96, 994, 2947, 138, 1499, 2, 96, 3, 107, 60, 89, 2423, 2, 96, 5948, 10, 2, 3, 87, 2, 3, 87, 35, 5, 29474, 5, 1677, 2, 3, 87, 17193, 2, 3, 87, 371, 2256, 834, 1074, 2, 96, 3155, 371, 2256, 2048, 2, 3, 87, 9, 3155, 2, 6397, 3155, 2, 3, 29, 2, 3, 87, 102, 3155, 2, 3, 87, 8481, 3155, 121, 2, 42, 2, 96, 23728, 1265, 52, 40, 121, 10, 121, 5948, 7, 10, 2, 3, 87, 2, 3, 87, 1986, 5, 17193, 4067, 5, 287, 2, 3, 87, 8221, 7, 2, 3, 13311, 2, 3, 87, 2577, 2, 3, 87, 7175, 8184, 940, 7, 7820, 17128, 5, 102, 1725, 121, 976, 12911, 1265, 52, 40, 121, 10, 121, 5948, 7, 10, 2, 3, 87, 2, 3, 87, 1986, 5, 17193, 4067, 5, 287, 2, 3, 87, 8221, 7, 2, 3, 13311, 2, 3, 87, 2577, 2, 3, 87, 7175, 8184, 940, 7, 7820, 17128, 5, 102, 1725, 121, 976, 23728, 518, 23, 26, 189, 121, 10, 25991, 976, 23728, 3845, 2632, 121, 10, 3288, 591, 976, 12911, 518, 23, 26, 189, 121, 10, 28212, 976, 12911, 3845, 2632, 121, 10, 4122, 14912, 2577, 3436, 2534, 2577, 3436, 2534, 976, 2176, 35, 7, 53, 121, 10, 121, 2, 3, 8481, 853, 2423, 2, 96, 51, 210, 18, 1893, 7, 49, 18, 670, 2562, 2, 96, 3155, 2, 3, 102, 3155, 196, 8307, 48, 18721, 13, 46, 3054, 6705, 5, 2, 3, 29, 2, 3, 87, 102, 3155, 2, 3, 102, 3155, 434, 447, 5167, 10, 2, 3, 9, 8318, 2423, 2, 96, 29, 32, 25278, 2, 96, 853, 2423, 2, 96, 994, 2947, 138, 1499, 2, 96, 3, 107, 60, 89, 2423, 2, 96, 5948, 10, 2, 3, 87, 2, 3, 87, 35, 5, 29474, 5, 1677, 2, 3, 87, 17193, 2, 3, 87, 371, 2256, 834, 1074, 2, 96, 3155, 371, 2256, 2048, 2, 3, 87, 9, 3155, 2, 6397, 3155, 2, 3, 29, 2, 3, 87, 102, 3155, 2, 3, 87, 8481, 3155, 121, 2, 3, 5, 32099, 1]\n",
      "Actual input: Slide the Block Messages switch to<unk> \"smallUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /1<unk> /15<unk> /Iphoneswitchonicon1.png\",\"bigUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /1<unk> /15<unk> /Iphoneswitchonicon1.png\",\"smallWidth\":460,\"smallHeight\":300,\"bigWidth\":760,\"bigHeight\":495.6521739130435,\"licensing\":\"<unk> div class=<unk> \"mw-parser-output<unk> \"><unk> p>I edited this screenshot of an iPhone<unk> n<unk> /p><unk> p>License:<unk> a rel=<unk> \"nofollow<unk> \" class=<unk> \"external text<unk> \" href=<unk> \"http:<unk> /<unk> /en.wikipedia.org<unk> /wiki<unk> /Fair_use<unk> \">Fair Use<unk> /a><unk> br><unk> n<unk> /p><unk> /div>\"<unk> or<unk> \"smallUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /2<unk> /28<unk> /Android7switchon.png\",\"bigUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /2<unk> /28<unk> /Android7switchon.png\",\"smallWidth\":460,\"smallHeight\":394,\"bigWidth\":760,\"bigHeight\":651.4285714285714,\"licensing\":\"<unk> div class=<unk> \"mw-parser-output<unk> \"><unk> p>I edited this screenshot of an Android icon.<unk> n<unk> /p><unk> p>License:<unk> a rel=<unk> \"nofollow<unk> \" class=<unk> \"external text<unk> \" href=<unk> \"http:<unk> /<unk> /en.wikipedia.org<unk> /wiki<unk> /Fair_use<unk> \">Fair Use<unk> /a><unk> br><unk> n<unk> /p><unk> /div>\"<unk>.<extra_id_0></s>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_tokenized_inputs = [(tokenizer.encode(input)) for input in encoded_dataset['train']['inputs']]\n",
    "longest_input = all_tokenized_inputs[np.argmax([len(input) for input in all_tokenized_inputs])]\n",
    "print(f'Longest input: {longest_input}')\n",
    "#Print the actual input\n",
    "print(f'Actual input: {tokenizer.decode(longest_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824b9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual input: Slide the Block Messages switch to<unk> \"smallUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /1<unk> /15<unk> /Iphoneswitchonicon1.png\",\"bigUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /1<unk> /15<unk> /Iphoneswitchonicon1.png\",\"smallWidth\":460,\"smallHeight\":300,\"bigWidth\":760,\"bigHeight\":495.6521739130435,\"licensing\":\"<unk> div class=<unk> \"mw-parser-output<unk> \"><unk> p>I edited this screenshot of an iPhone<unk> n<unk> /p><unk> p>License:<unk> a rel=<unk> \"nofollow<unk> \" class=<unk> \"external text<unk> \" href=<unk> \"http:<unk> /<unk> /en.wikipedia.org<unk> /wiki<unk> /Fair_use<unk> \">Fair Use<unk> /a><unk> br><unk> n<unk> /p><unk> /div>\"<unk> or<unk> \"smallUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /2<unk> /28<unk> /Android7switchon.png\",\"bigUrl\":\"https:<unk> /<unk> /www.wikihow.com<unk> /images<unk> /2<unk> /28<unk> /Android7switchon.png\",\"smallWidth\":460,\"smallHeight\":394,\"bigWidth\":760,\"bigHeight\":651.4285714285714,\"licensing\":\"<unk> div class=<unk> \"mw-parser-output<unk> \"><unk> p>I edited this screenshot of an Android icon.<unk> n<unk> /p><unk> p>License:<unk> a rel=<unk> \"nofollow<unk> \" class=<unk> \"external text<unk> \" href=<unk> \"http:<unk> /<unk> /en.wikipedia.org<unk> /wiki<unk> /Fair_use<unk> \">Fair Use<unk> /a><unk> br><unk> n<unk> /p><unk> /div>\"<unk>.<extra_id_0></s>\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual input: {tokenizer.decode(longest_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ccedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c80ea08d614b92874d83b4f8b6fd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 35 at dim 1 (got 277)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Create the Trainer and train\u001b[39;00m\n\u001b[0;32m     23\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     24\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     25\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     26\u001b[0m     train_dataset\u001b[39m=\u001b[39mencoded_dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     27\u001b[0m     eval_dataset\u001b[39m=\u001b[39mencoded_dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1899\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1900\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1901\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32md:\\digit\\Documents\\UMass\\CS685\\FinalProject\\nlp-project\\.venv\\lib\\site-packages\\transformers\\data\\data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[0;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 35 at dim 1 (got 277)"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the more memory used)\n",
    "    prediction_loss_only=True, # If I should only return the loss\n",
    "    learning_rate=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create the Trainer and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a73bf1d-e514-4ff9-bde1-d27d646aef21",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8bfed-2dca-4893-a2f2-0057fd12b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast = True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config = config,\n",
    "    from_tf=bool('.ckpt' in model_name_or_path)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c17b3663-032d-4050-a83c-594874a01dcc",
   "metadata": {},
   "source": [
    "## Tokenize the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1800c1-52fa-4c30-9599-954f6fff6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['inputs']]\n",
    "    targets = [ex for ex in examples['targets']]\n",
    "    model_inputs = tokenizer(inputs, max_length= data_args['max_source_length'], padding = False, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length = data_args['max_target_length'], padding = False, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bad4c8-8d2d-4598-b566-365dee7f4e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = datasets['train'], datasets['validation']\n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "698ec022-79d7-48c9-946d-4d2379305abc",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ad8ec-32f7-4845-a596-08f6d9a485bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "label_pad_token_id = -100 if data_args['ignore_pad_token_for_loss'] else tokenizer.pad_token_id\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, label_pad_token_id=label_pad_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6fc2d21-67db-4ca3-a1ff-2a13d3a08081",
   "metadata": {},
   "source": [
    "### Initialise Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af078d7-a039-4f7a-93a1-bb49d35d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfomers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8065fd90-776e-4a87-af55-8cb4e1ac0866",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947cb35-9f6a-4ff0-bb22-7cf87c1872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_result = trainer.train(model_path=model_name_or_path if os.path.isdir(model_name_or_path) else None)\n",
    "trainer.save_model()\n",
    "\n",
    "output_train_file = os.path.join(training_args['output_dir'], 'train_results.txt')\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_train_file, 'w') as writer:\n",
    "        for key, value in sorted(train_result.metrics.items()):\n",
    "            writer.write(f'{key} = {value}\\n')\n",
    "\n",
    "    # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n",
    "    trainer.state.save_to_json(os.path.join(training_args['output_dir'], 'trainer_state.json'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "795e4184-03cc-4e21-b777-bcc0118cf253",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccde40-d025-4b44-913c-4007cc6d8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate_predictions(pred_filename, gold_filename):\n",
    "    with open(pred_filename, 'r') as pred_f, open(gold_filename) as gold_f:\n",
    "        pred_lines = pred_f.readlines()\n",
    "        gold_lines = gold_f.readlines()\n",
    "    \n",
    "        total = 0.0\n",
    "        full_correct = 0.0\n",
    "        first_correct = 0.0\n",
    "        \n",
    "        for i in range(len(pred_lines)):\n",
    "            pred_line = pred_lines[i].strip()\n",
    "            if gold_filename.endswith('.json'):\n",
    "                gold_json = json.loads(gold_lines[i])\n",
    "                gold_line = gold_json['translation']['tgt']\n",
    "            else:  \n",
    "                gold_line = gold_lines[i].strip().split('\\t')[1]\n",
    "            \n",
    "            # remove space before period/question mark\n",
    "            gold_line = gold_line.replace(' ?', '?').replace(' .', '.').replace(' ,', ',') \n",
    "\n",
    "            total +=1\n",
    "\n",
    "            if pred_line == gold_line:\n",
    "                full_correct += 1\n",
    "                first_correct += 1\n",
    "            else:\n",
    "                pred_words = pred_line.split()\n",
    "                gold_words = gold_line.split()\n",
    "                if len(pred_words) > 0 and pred_words[0] == gold_words[0]:\n",
    "                    first_correct += 1\n",
    "\n",
    "  \n",
    "    return  (first_correct / total), (full_correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7ef12-1836-4b06-bbb8-ce6de1c654a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = os.path.basename(data_args['validation_file']).replace('.json', '')\n",
    "\n",
    "predictions = trainer.predict(test_dataset=eval_dataset, max_length=100)\n",
    "output_pred_file = os.path.join(training_args['output_dir'], basename + '.eval_preds_seq2seq.txt')\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_pred_file, 'w') as writer:\n",
    "        for pred in tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True):\n",
    "            writer.write(pred + '\\n')\n",
    "\n",
    "output_eval_file = os.path.join(training_args['output_dir'], basename + '.eval_results_seq2seq.txt')\n",
    "first_acc, full_acc = evaluate_predictions(output_pred_file, data_args['validation_file'])\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_eval_file, 'w') as writer:\n",
    "        writer.write(f'Exact match accuracy: {full_acc}\\n')\n",
    "        writer.write(f'First word accuracy: {first_acc}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
